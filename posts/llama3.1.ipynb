{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "797bd226-ae00-491a-b312-3b55c4418911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a848c7af82e54c8699c6eaa50bcffb27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "torch.backends.quantized.engine = \"qnnpack\"\n",
    "# Load the pre-trained model and tokenizer\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Apply dynamic quantization to the model\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# Create a pipeline with the quantized model\n",
    "quantized_pipeline = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=quantized_model, \n",
    "    tokenizer=tokenizer,\n",
    "    device = -1\n",
    ")\n",
    "\n",
    "normal_pipeline = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    device = -1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9a41a7d-fdf9-457b-a676-5c11e7c41883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 30633.023681640625\n",
      "quantized 2005.023681640625\n"
     ]
    }
   ],
   "source": [
    "def get_model_size(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.numel() * param.element_size()\n",
    "\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.numel() * buffer.element_size()\n",
    "\n",
    "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "    return size_all_mb\n",
    "\n",
    "print('model', get_model_size(model))\n",
    "print('quantized', get_model_size(quantized_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "390e9677-e49c-4490-bbe7-25db3a9e208a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"Tell a dad joke that involves socks with sandals\\nHere's one:\\n\\nWhy did the sock go to the party with the sandal?\\n\\nBecause it was a sole-ful occasion! (get it? sole, like the bottom of the foot,\"}]\n",
      "[{'generated_text': \"Tell a dad joke that involves socks with sandals\\nHere's one: Why did the sock go with the sandal? Because it was a sole-ful match! (get it? sole-ful, like soulful, but also a reference\"}]\n"
     ]
    }
   ],
   "source": [
    "def test_pipeline(pipe, prompt=\"Tell a dad joke that involves socks with sandals\"):\n",
    "    # Test the quantized model\n",
    "    output = pipe(prompt, max_length=50)\n",
    "    print(output)\n",
    "test_pipeline(quantized_pipeline)\n",
    "test_pipeline(normal_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42447a0a-32ee-4fb1-8502-702ceaec42aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
