[
  {
    "objectID": "CLAUDE.html",
    "href": "CLAUDE.html",
    "title": "jjd.io - Personal Website Project",
    "section": "",
    "text": "This is Justin Donaldson’s personal website built with Quarto, showcasing professional background, blog posts, CV, and project portfolio.\n\n\n\n\nTechnology: Quarto static site generator\nDomain: jjd.io\nHosting: GitHub Pages (via CNAME file)\nTheme: Sandstone theme with custom CSS\n\n\n\n\n\n_quarto.yml: Main configuration file\nindex.qmd: About/homepage content\ncv.qmd: Curriculum Vitae page\nblog.qmd: Blog listing page\nposts/: Blog post content (mix of .qmd and .ipynb files)\nimages/: Site images and photos\nstyle.css: Custom styling\n\n\n\n\n\nTone: Professional but approachable, academic background with industry focus\nFocus: AI/ML, data science, visualization, human-computer interaction\nUpdates: Keep current role (Curvo Labs) prominent, past roles in chronological order\nBlog topics: Technical posts about AI/ML, data science tutorials, industry insights\n\n\n\n\n\nTeam photos should include proper attribution\nProfessional headshots and company logos in /images/\nBlog post images in /posts/images/\n\n\n\n\n\nUses Quarto’s freeze feature for computational content\nJupyter notebooks (.ipynb) and Quarto markdown (.qmd) supported\nSite builds to _site/ directory\nGoogle Analytics tracking enabled\n\n\n\n\n\nGitHub Actions: Automated deployment via .github/workflows/main.yml\nTrigger: Pushes to main branch automatically deploy\nProcess: Quarto render → GitHub Pages publish to gh-pages branch\nFeatures:\n\nTinyTeX for LaTeX/PDF support\nWeasyPrint for PDF CV generation\nReady for Python/R/Julia dependencies if needed\n\nDomain: Deploys to jjd.io via CNAME file\n\n\n\n\n\nUpdate CV with new roles, publications, patents\nAdd new blog posts in /posts/\nUpdate team photos and professional images\nKeep bibliography files current in /bibliographies/\n\n\n\n\n\nRole: Chief Data Scientist at Curvo Labs (2024-present)\nPrevious: Hushh.ai CTO, Salesforce Principal Data Scientist/Engineer\nRecent additions: Curvo Labs team photo, role updates completed\n\n\n\n\n\nQuarto\nPython (for Jupyter notebooks)\nR (for some content)\nPolars preferred for data analysis examples\n© Copyright 2024 Justin Donaldson. Except where otherwise noted, all rights reserved. The views and opinions on this website are my own and do not represent my current or former employers."
  },
  {
    "objectID": "CLAUDE.html#project-overview",
    "href": "CLAUDE.html#project-overview",
    "title": "jjd.io - Personal Website Project",
    "section": "",
    "text": "This is Justin Donaldson’s personal website built with Quarto, showcasing professional background, blog posts, CV, and project portfolio."
  },
  {
    "objectID": "CLAUDE.html#site-structure",
    "href": "CLAUDE.html#site-structure",
    "title": "jjd.io - Personal Website Project",
    "section": "",
    "text": "Technology: Quarto static site generator\nDomain: jjd.io\nHosting: GitHub Pages (via CNAME file)\nTheme: Sandstone theme with custom CSS"
  },
  {
    "objectID": "CLAUDE.html#key-files",
    "href": "CLAUDE.html#key-files",
    "title": "jjd.io - Personal Website Project",
    "section": "",
    "text": "_quarto.yml: Main configuration file\nindex.qmd: About/homepage content\ncv.qmd: Curriculum Vitae page\nblog.qmd: Blog listing page\nposts/: Blog post content (mix of .qmd and .ipynb files)\nimages/: Site images and photos\nstyle.css: Custom styling"
  },
  {
    "objectID": "CLAUDE.html#content-guidelines",
    "href": "CLAUDE.html#content-guidelines",
    "title": "jjd.io - Personal Website Project",
    "section": "",
    "text": "Tone: Professional but approachable, academic background with industry focus\nFocus: AI/ML, data science, visualization, human-computer interaction\nUpdates: Keep current role (Curvo Labs) prominent, past roles in chronological order\nBlog topics: Technical posts about AI/ML, data science tutorials, industry insights"
  },
  {
    "objectID": "CLAUDE.html#image-management",
    "href": "CLAUDE.html#image-management",
    "title": "jjd.io - Personal Website Project",
    "section": "",
    "text": "Team photos should include proper attribution\nProfessional headshots and company logos in /images/\nBlog post images in /posts/images/"
  },
  {
    "objectID": "CLAUDE.html#development-notes",
    "href": "CLAUDE.html#development-notes",
    "title": "jjd.io - Personal Website Project",
    "section": "",
    "text": "Uses Quarto’s freeze feature for computational content\nJupyter notebooks (.ipynb) and Quarto markdown (.qmd) supported\nSite builds to _site/ directory\nGoogle Analytics tracking enabled"
  },
  {
    "objectID": "CLAUDE.html#deployment",
    "href": "CLAUDE.html#deployment",
    "title": "jjd.io - Personal Website Project",
    "section": "",
    "text": "GitHub Actions: Automated deployment via .github/workflows/main.yml\nTrigger: Pushes to main branch automatically deploy\nProcess: Quarto render → GitHub Pages publish to gh-pages branch\nFeatures:\n\nTinyTeX for LaTeX/PDF support\nWeasyPrint for PDF CV generation\nReady for Python/R/Julia dependencies if needed\n\nDomain: Deploys to jjd.io via CNAME file"
  },
  {
    "objectID": "CLAUDE.html#maintenance-tasks",
    "href": "CLAUDE.html#maintenance-tasks",
    "title": "jjd.io - Personal Website Project",
    "section": "",
    "text": "Update CV with new roles, publications, patents\nAdd new blog posts in /posts/\nUpdate team photos and professional images\nKeep bibliography files current in /bibliographies/"
  },
  {
    "objectID": "CLAUDE.html#current-status",
    "href": "CLAUDE.html#current-status",
    "title": "jjd.io - Personal Website Project",
    "section": "",
    "text": "Role: Chief Data Scientist at Curvo Labs (2024-present)\nPrevious: Hushh.ai CTO, Salesforce Principal Data Scientist/Engineer\nRecent additions: Curvo Labs team photo, role updates completed"
  },
  {
    "objectID": "CLAUDE.html#dependencies",
    "href": "CLAUDE.html#dependencies",
    "title": "jjd.io - Personal Website Project",
    "section": "",
    "text": "Quarto\nPython (for Jupyter notebooks)\nR (for some content)\nPolars preferred for data analysis examples"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "© Copyright 2024 Justin Donaldson. Except where otherwise noted, all rights reserved. The views and opinions on this website are my own and do not represent my current or former employers."
  },
  {
    "objectID": "cv.html#skills",
    "href": "cv.html#skills",
    "title": "Curriculum Vitae",
    "section": "Skills",
    "text": "Skills\n\nLarge Language Model Research and Applications\nData Science and Advanced Machine Learning\nModeling, Model Optimization, Time Series, Anomaly Detection\nAdvanced ML/Web Visualization (Embedding/ML/SVG Related)\nInteraction Design, Prototyping, and Design Theory\nCustomer Service Analysis\nProgramming Language Design\nDocker, Python, R, + most core web technologies and languages."
  },
  {
    "objectID": "cv.html#experience",
    "href": "cv.html#experience",
    "title": "Curriculum Vitae",
    "section": "Experience",
    "text": "Experience\nCurvo Labs Chief Data Scientist 2024-\nLeadership and Engineering Highlights:\n\nLeading data science initiatives and research.\nDeveloping advanced machine learning solutions.\n\nHushh.ai Cofounder 2024\nLeadership and Engineering Highlights:\n\nDeveloped strategies for on-device hybrid LLM search and modeling.\nCoordinated and managed a large team of R&D interns.\n\nSalesforce.com Principal Data Scientist/Engineer Service Cloud 2014-2023\nData Science and Engineering Highlights:\n\nFirst Senior Search Datascientist.\nMigrated the core search infrastructure to model-trained ranking coefficients from their previous ad hoc system of boosts and filters.\nDeveloped and patented deep learning models for predicting integration test failures, and error assignments based on code changes.\nCo-founding member of Service Cloud Datascience team.\nPart of a small team of data scientists tasked with creating an ML education program for all Salesforce engineers.\nProduced the initial Docker Container configuration and serving code that is used as the basis for all production AWS Sagemaker deployments.\nCo-founding member of Foundational AI Components team that produced re-usable libraries and components for advanced ML services.\nMember of joint Engineering/Research Leadership Team responsible for investigating and developing related GPT-3 technologies.\n\nC-level projects with Trusted Salesforce Partners:\n\nWrote a custom job candidate scoring function for CPL, Ireland’s largest recruitment firm.\nWrote an advanced candidate profiler for Allegis, The fourth largest recruitment firm in the world.\nWrote an anomaly detection model for Hulu to detect when an isolated event was affecting global customer call volume to an unsustainable degree.\nWorked with Citibank data center team to combine customer financial activity with service interactions in order to drive predictive customer issue resolution.\n\nBigML Co-founder/President 2011-2013\nLeadership and Engineering Highlights\n\nDeveloped and patented interactive visualizations used to represent and explore trained models and distributions.\nManaged various administrative tasks as President (Insurance, Expenses, Office management, etc.)\nLed development of website, and integration with internal/external APIs."
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Curriculum Vitae",
    "section": "Education",
    "text": "Education\nIndiana University - Bloomington, Indiana - 2011\nPh.D, Informatics - January 2011\nMy Erdös Number is 4:\n\nPaul Erdös\nFan Chung\nAlessandro Vespignani\nFilippo Menczer\n(me)\n\nDissertation: “Visualization of music relational information sources for analysis, navigation, and discovery”\nAdvising Committee : Erik A. Stolterman (Chair), Filippo Menczer, Jon Paolillo, Jeff Bardzell, Donald Byrd, Marc Torrens.\nPresident of Graduate Student Body (School of Informatics)\nBest Paper Award (J. Donaldson 2006b)\nIndiana University - Bloomington, Indiana - 2006\nM.S., Human Computer Interaction Design\nDepauw University - Greencastle, Indiana - 2001\nB.S., Computer Science (Minor in Mathematics)"
  },
  {
    "objectID": "cv.html#publications",
    "href": "cv.html#publications",
    "title": "Curriculum Vitae",
    "section": "Publications",
    "text": "Publications\n\n\nBaccigalupo, Claudio, Enric Plaza, and Justin Donaldson. 2008. “Uncovering Affinity of Artists to Multiple Genres from Social Behaviour Data.” In ISMIR, 275–80.\n\n\nDonaldson, Justin. 2006a. “Limestick: Designing for Performer-Audience Connection in Laptop Based Computer Music.” In CHI ’06: CHI ’06 Extended Abstracts on Human Factors in Computing Systems, 712–17. New York, NY, USA: ACM. https://doi.org/http://doi.acm.org/10.1145/1125451.1125595.\n\n\n———. 2006b. “ZMDS: Visualizing Structural Entropy in Queried Network Sub-Graphs.” In. Netsci ’06. Bloomington, IN, USA: Association for Computing Machinery. http://vw.indiana.edu/netsci06/conference/Donaldson_ZMDS_.pdf.\n\n\n———. 2007a. “A Hybrid Social-Acoustic Recommendation System for Popular Music.” In Proceedings of the 2007 ACM Conference on Recommender Systems, 187–90. RecSys ’07. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/1297231.1297271.\n\n\n———. 2007b. “Music Recommendation Mapping and Interface Based on Structural Network Entropy.” In 2007 IEEE 23rd International Conference on Data Engineering Workshop, 811–17. IEEE.\n\n\n———. 2011. “Visualization of Music Relational Information Sources for Analysis, Navigation, and Discovery.” PhD thesis, Indiana University.\n\n\nDonaldson, Justin J, Michael Conover, Benjamin Markines, Heather Roinestad, and Filippo Menczer. 2008. “Visualizing Social Links in Exploratory Search.” In Proceedings of the Nineteenth ACM Conference on Hypertext and Hypermedia, 213–18.\n\n\nDonaldson, Justin, Joshua Evnin, and Sidharth Saxena. 2005. “ECHOES: Encouraging Companionship, Home Organization, and Entertainment in Seniors.” In CHI’05 Extended Abstracts on Human Factors in Computing Systems, 2084–88.\n\n\nDonaldson, Justin, Alla Genkina, Scott MacArthur, Muzaffer Ozakca, and Amanda Stephano. 2004. “DOVE: Digital Olympic Voting Environment.” In CHI’04 Extended Abstracts on Human Factors in Computing Systems, 1631–35.\n\n\nDonaldson, Justin, and William Hazlewood. 2008. “Candidate Mapping: Finding Your Place Amongst the Candidates.” In 2008 12th International Conference Information Visualisation, 63–68. IEEE.\n\n\nDonaldson, Justin, Ian Knopke, and Chris Raphael. 2007. “Chroma Palette: Chromatic Maps of Sound as Granular Synthesis Interface.” In Proceedings of the 7th International Conference on New Interfaces for Musical Expression, 213–18.\n\n\nDonaldson, Justin, and Paul Lamere. 2009. “Using Visualizations for Music Discovery.” Tutorial at ISMIR 9: 99.\n\n\nLim, Youn-kyung, Justin Donaldson, Heekyoung Jung, Breanne Kunz, David Royer, Shruti Ramalingam, Sindhia Thirumaran, and Erik Stolterman. 2008. “Emotional Experience and Interaction Design.” In Affect and Emotion in Human-Computer Interaction: From Theory to Applications, edited by Christian Peter and Russell Beale, 116–29. Berlin, Heidelberg: Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-540-85099-1_10.\n\n\nLim, Youn-kyung, Erik Stolterman, Heekyoung Jung, and Justin Donaldson. 2007a. “Interaction Gestalt and the Design of Aesthetic Interactions.” In Proceedings of the 2007 Conference on Designing Pleasurable Products and Interfaces, 239–54. DPPI ’07. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/1314161.1314183.\n\n\n———. 2007b. “Interaction Gestalt and the Design of Aesthetic Interactions.” In Proceedings of the 2007 Conference on Designing Pleasurable Products and Interfaces, 239–54.\n\n\nMartin, Francisco J., Justin Donaldson, Adam Ashenfelter, Marc Torrens, and Rick Hangartner. 2011. “The Big Promise of Recommender Systems.” AI Magazine 32 (3): 19–27. https://doi.org/10.1609/aimag.v32i3.2360."
  },
  {
    "objectID": "cv.html#patents",
    "href": "cv.html#patents",
    "title": "Curriculum Vitae",
    "section": "Patents",
    "text": "Patents\nJustin Donaldson. Real-time visualization of user consumption of media items. US8332406B2\nJ. Justin Donaldson, Adam Ashenfelter, Francisco Martin, Jos Verwoerd, Jose Antonio Ortega, Charles Parker. Visualization and interaction with compact representations of decision trees.\nUS20200379951A1\nFrancisco J. Martin, Adam Ashenfelter, J. Justin Donaldson, Jos Verwoerd, Jose Antonio Ortega, Charles Parker. Evolving parallel system to automatically improve the performance of multiple concurrent tasks on large datasets. US9558036B1\nFrancisco J. Martin, Oscar Rovira, Jos Verwoerd, Poul Petersen, Charles Parker, Jose Antonio Ortega, Beatriz Garcia, J. Justin Donaldson, Antonio Blasco, Adam Ashenfelter. Predictive modeling and data analysis in a secure shared system. US20170140302A1\nJ. Justin Donaldson, Benjamin Busjaeger, Siddharth Rajaram, Berk Coker, Hormoz Tarevern. Machine learning based ranking of test cases for software development. US10474562B2\nJustin Donaldson. Personal music recommendation mapping WO2008051882A3\nJ. Justin Donaldson, Hormoz Tarevern, Sadiya Hameed, Siddharth Srivastava, Feifei Jiang. Error assignment for computer programs. US10409667B2\nZachary Alexander, Scott Thurston Rickard, Jr., Clifford Z. Huang, J. Justin Donaldson. Accounting for positional bias in a document retrieval system using machine learning. US10565265B2\nScott Thurston Rickard, Jr., Clifford Z. Huang, J. Justin Donaldson. Adjusting feature weights for ranking entity based search results. US20180052853A1"
  },
  {
    "objectID": "cv.html#pro-bono",
    "href": "cv.html#pro-bono",
    "title": "Curriculum Vitae",
    "section": "Pro Bono",
    "text": "Pro Bono\nUniversity of Washington - Instructor/Contributor\n\nCo-Instructor for UW 410 Advanced Machine Learning.\n(Slides Available: https://jdonaldson.github.io/uw-mlearn410/)\nMember of UW Advisory Board for their school of Professional and Continuing Education\n\nHaxe Foundation - Contributor/Member\n\nDeveloped the Lua target for the Haxe language blog announcement | hacker news fp\nSponsored Summer-of-Code for Haxe programmers through mentorship and financial contributions.\n\nUCLA Graduate School - Data Science Advisor/Mentor\n\nCo-hosted a graduate student Data Science competition in collaboration with Boston Day Academy (Slides Available : https://is.gd/qnji8A)"
  },
  {
    "objectID": "posts/quarto.html",
    "href": "posts/quarto.html",
    "title": "Blogging with Quarto",
    "section": "",
    "text": "Quarto Example\n\n\nI’m creating a new blog, and was surveying the various options. I’ve been using the default Jekyll “Pages” functionality offered from Github for years, but it’s time to move to something more modern.\nI came across Quarto from a recent post by Jeremy Howard, and was immediately sold. Quarto has its lineage somewhat in the realm of Tex, which is an ancient text setting program written by Donald Knuth, one of the early fathers of computer programming. The Tex computer program eventually evolved into LaTex, which has a scholarly citation management system called BibTeX. Quarto has a mechanism to use these scholarly bibliographic formats, like the ones I maintain for myself when I was writing my dissertation.\nHowever, instead of using the somewhat obtuse LaTex syntax, Quarto uses variations on Markdown, which is a much simpler plain text format for creating basic formatted text. The “qmd” format is easy enough to type out manually with no editor other than a basic terminal (which is also what I use to write these posts). You can see the basic qmd format for this post here.\nI find that I don’t need to write as many mathematical formulas here. Although Quarto enables that easily as well. Here’s one of my favorites, the formula for entropy in information theory.\n\\[H(X) = -\\sum_{i=1}^n p(x_i) \\log p(x_i)\\]\nI can also draw some simple diagrams using a builtin Mermaid syntax:\n\n\n\n\n\nflowchart LR\n  A[Hard edge] --&gt; B(Round edge)\n  B --&gt; C{Decision}\n  C --&gt; D[Result one]\n  C --&gt; E[Result two]\n\n\n\n\n\n\nI can also easily embed youtube clips:\n\nAll in all, most of the basic WYSIWYG formatting work is handled through simple text specification. There’s ways to tweak things even more using css, but for now I’m happy with some basic defaults and theming.\nThere’s two main reasons why I think it’s worth using Quarto:\n\nIt’s Free\nIt doesn’t cost a dime to use Quarto. I simply took the time to set it up with my free Github account, and use it to publish my site free on Github as well.\nNow, there are some that will argue that Quarto is not at all free, in fact it costs time to set this up and write things yourself.\nHowever, I would also argue:\n\n\nIt Has a High “Give a S@&$! Factor”\nWe’re entering an age where most text written online will be generated or influenced by a generative AI. This post has been written with the help of one. However, I believe that human communication requires some actual proof of work… some way of showing that the author cares about the topic. It’s going to get very difficult to do that unless you structure your communication in a way that shows a deep level of interest in a topic, and not just churning out one-off dross for the sake of posting to Linkedin.\nThe main reason isn’t that it’s cheap or shows you care though, it’s because you should own your story as long as you can, and you will probably outlive any company that tries to do too many things for you. This brings up the next advantage of Quarto:\n\n\nIt’s Open Source and Lindy\nTechnology has been around long enough that it should be clear that languages, frameworks, and applications come and go along with the companies that invent them. The concept of the Lindy effect argues that future life expectancy of some non-perishable thing (like an idea or technology) is proportional to its age. Right now, Quarto is built on top of some of the oldest ideas in all of computer theory, and it’s editable and usable inside my Neovim editor, which itself is based off of the VI editor from 1976, using the QWERTY keyboard layout from 1878.\nNone of these technologies are going to go anywhere. They’re good bets for writing things down that you want to control and preserve.\n\n\nThanks for Reading!\nI hope I gave a good overview of what Quarto is about, and why it matters. Thanks for taking the time to read. Hoping to see your Quarto site out there some day!\n\n\n\n\n© Copyright 2024 Justin Donaldson. Except where otherwise noted, all rights reserved. The views and opinions on this website are my own and do not represent my current or former employers."
  },
  {
    "objectID": "posts/intelligence_chases_chaos.html",
    "href": "posts/intelligence_chases_chaos.html",
    "title": "Intelligence Chases Chaos",
    "section": "",
    "text": "Intelligence Chases Chaos\nFractals have an astonishing property: their dimensionality is only a fraction of the space they inhabit. The paths of hurricanes, the spread of wildfires, the beating of the human heart—all of these chaotic systems play out in a space far smaller than their theoretical possibilities.\nOne way of better understanding fractal dimensionality is to understand that it’s not a dimension at all, it’s way of understanding how dimensions change through scaling.\nA good way of understanding this phenomenon is to try and measure a coastline.\nAs one shrinks the resolution of their measurement of a coastline, the length one measures will change, but the amount of change is typically bounded within a given range.\n© Copyright 2024 Justin Donaldson. Except where otherwise noted, all rights reserved. The views and opinions on this website are my own and do not represent my current or former employers."
  },
  {
    "objectID": "posts/intelligence_chases_chaos.html#neural-network-fractals",
    "href": "posts/intelligence_chases_chaos.html#neural-network-fractals",
    "title": "Intelligence Chases Chaos",
    "section": "Neural Network Fractals",
    "text": "Neural Network Fractals\nNeural networks, especially deep learning architectures, actually exhibit fractal-like properties in their information processing and representational spaces. Just as fractals demonstrate non-integer dimensionality by creating complex structures through recursive self-similarity, neural networks generate intricate representations by layering increasingly abstract and self-similar feature transformations.\nConsider convolutional neural networks (CNNs) as an example. As information passes through successive layers, the network progressively extracts features at different scales and levels of abstraction - much like how a fractal reveals different geometric patterns when you zoom in or out. Each layer can be thought of as a kind of “scaling” operation where local patterns are transformed into more global, complex representations. It is a sort of map of the chaos that exists at different levels of abstraction. And, it’s clear that the model is learning some of the same patterns at different levels of abstraction.\n\n\n\nNeural Feature Hierarchy\n\n\nThe fractal-like behavior emerges most clearly in how neural networks handle high-dimensional data. When processing complex inputs like images or language, networks don’t simply map inputs to outputs linearly. Instead, they create multi-dimensional, non-linear transformation spaces where local similarities and global structures coexist - precisely the kind of space where fractal mathematics becomes compelling.\nResearchers like Geoffrey Hinton have even suggested that neural network “feature spaces” might be better understood through fractal geometry than traditional Euclidean approaches. The networks generate representations that are neither purely low-dimensional nor simply high-dimensional, but exist in a kind of fractal in-between state. He gives some of his most recent insight on it here\nMore concretely, techniques like fractal interpolation and fractal compression have direct computational analogues in how neural networks learn and generalize. Both involve recursive strategies for capturing complex patterns at multiple scales of resolution. This holds for images and text in several senses. Transformer models can “hallucinate” parts of an image at any scale, and language models can condense or expand text handily through summary or extrapolation."
  },
  {
    "objectID": "posts/intelligence_chases_chaos.html#the-fractal-shortcut-to-understanding",
    "href": "posts/intelligence_chases_chaos.html#the-fractal-shortcut-to-understanding",
    "title": "Intelligence Chases Chaos",
    "section": "The Fractal Shortcut to Understanding",
    "text": "The Fractal Shortcut to Understanding\nArtificial Intelligence is often accused of being “brute force,” of needing endless data to approximate human intuition. But intelligence—biological or artificial—works because the world isn’t as high-dimensional as it seems. The constraints of physics, economics, and even human behavior force events to unfold along surprisingly low-dimensional manifolds. Chaos, for all its turbulence, is confined to a fractal space. While there may be “No Free Lunch” from a pure statistical point of view, we thankfully live in a statistical world that happens to also be chaotic.\nThis is why deep learning models, trained on quintessentially chaotic market systems, can predict stock trends better than random guessing. It’s why language models, given enough text, can anticipate our next words with uncanny accuracy. They aren’t solving the problem in full dimensionality—they’re following the fractal contours where reality actually unfolds."
  },
  {
    "objectID": "posts/intelligence_chases_chaos.html#ai-as-the-ultimate-chaos-hunter",
    "href": "posts/intelligence_chases_chaos.html#ai-as-the-ultimate-chaos-hunter",
    "title": "Intelligence Chases Chaos",
    "section": "AI as the Ultimate Chaos Hunter",
    "text": "AI as the Ultimate Chaos Hunter\nIf intelligence is about making sense of complexity, then AI is the ultimate tool for chasing Chaos. Traditional physics tries to model the world with explicit equations, but AI sidesteps that limitation. It doesn’t need the equations or full rules of a system—it learns the shape of Chaos directly from observation. Neural networks extract the latent structure from raw events, distilling their fractal dimensionality into something even smaller: a compressed model of reality that understands the large and the small, and how they can relate.\nThis is why AI-driven weather prediction is overtaking classical models. Why AI in finance can detect patterns even seasoned traders miss. And why AI in science is accelerating discoveries faster than human intuition alone ever could.\nIntelligence—ours or artificial—isn’t ultimately about defeating Chaos. It’s about finding its shape in the areas we care about, and learning how to ride its back when necessary. Part of my goal this year is to integrate more fractal consideration of data in my projects this year."
  },
  {
    "objectID": "posts/semantic-proprioception-demo.html",
    "href": "posts/semantic-proprioception-demo.html",
    "title": "Semantic Proprioception: Teaching Data to Understand Itself",
    "section": "",
    "text": "Just as proprioception lets you sense where your body is in space without looking, semantic proprioception gives data the ability to understand its own internal structure. No manual labeling, no predefined categories—just the data revealing patterns within itself.\nI’ve built a live demo that shows this in action across three very different datasets: Twitter customer support conversations, ArXiv research papers, and Hacker News discussions. The same technique discovers meaningful themes in all three, adapting to each domain’s unique semantics.\n\n\nTraditional clustering requires you to specify how many clusters you want, tune distance thresholds, or provide seed examples. But what if the data could just tell you what patterns exist?\nThe key is LSH bucket density. When you hash similar embeddings into buckets using Locality-Sensitive Hashing, the density of each bucket reveals something fundamental.\nLSH maps high-dimensional vectors to binary signatures using random hyperplanes:\n\\[h_i(\\mathbf{v}) = \\begin{cases} 1 & \\text{if } \\mathbf{w}_i \\cdot \\mathbf{v} &gt; 0 \\\\ 0 & \\text{otherwise} \\end{cases}\\]\nwhere \\(\\mathbf{w}_i\\) is a random hyperplane. Combining \\(k\\) such hash functions creates a bucket signature. Similar vectors collide in the same bucket with high probability:\n\\[P(h(\\mathbf{u}) = h(\\mathbf{v})) = 1 - \\frac{\\theta(\\mathbf{u}, \\mathbf{v})}{\\pi}\\]\nwhere \\(\\theta(\\mathbf{u}, \\mathbf{v})\\) is the angle between vectors. Closer vectors (smaller angle) → higher collision probability.\nThe density distribution then tells us:\n\nDense buckets (≥5 items): Common themes, frequently occurring concepts\nMedium buckets (2-4 items): Boundary cases, transitional concepts\nSparse buckets (1 item): Novel or rare content\n\nThis isn’t just clustering—it’s the data developing awareness of its own distribution.\n\n\n\nTraditional LSH implementations have a problem: to find dense buckets, you’d have to scan every bucket and count items. That’s O(n) where n is the number of buckets—expensive and slow.\nEnter Krapivin hash tables (Krapivin et al. 2025), which provide O(1) density queries through their hierarchical structure. You can instantly ask: “Which buckets have ≥5 items?” without scanning anything.\nThis transforms LSH from a search index into a semantic awareness system. The data doesn’t just answer “what’s similar to X?”—it can answer “what patterns exist in me?”\n\n\n\nThe demo shows how the same approach works across wildly different domains:\n\n\nDiscovered themes: Password resets, billing issues, account access, network problems\nThe short, action-oriented nature of support tickets creates tight, well-defined clusters. Users express problems in similar ways, leading to high-density buckets around common pain points.\n\n\n\nDiscovered themes: Deep learning architectures, quantum mechanics, genomics, optimization methods\nAcademic writing has longer, more varied language, but technical concepts still cluster. Papers about “attention mechanisms” use similar terminology even when discussing different applications.\n\n\n\nDiscovered themes: AI/ML developments, startup advice, privacy concerns, programming tools\nHN posts mix news headlines with discussion text. The clusters reflect both trending topics and perennial themes in the tech community.\n\n\n\n\n\nEmbed: Use sentence-transformers to convert text → 384 or 768-dimensional vectors\nHash: Apply LSH with a fixed seed (12345) so embeddings from different files map to the same bucket space\nDiscover: Query Krapivin hash table for buckets with ≥5 items (O(1) operation)\nLabel: Use an LLM or keyword extraction to generate semantic labels for each dense bucket\nMerge: Combine similar themes using Jaccard similarity on tokenized labels:\n\n\\[J(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}\\]\nwhere \\(A\\) and \\(B\\) are sets of tokens from theme labels. Themes with \\(J \\geq 0.5\\) get merged automatically.\nAll embeddings are pre-computed (~24 MB total), so the demo runs with zero API costs or inference overhead.\n\n\n\nBecause we use a fixed LSH seed across all files, the bucket spaces are compatible. This means:\n\nAdd new data files → just compute their LSH signatures → merge with existing index\nRemove files → delete their entries from affected buckets\nQuery across multiple datasets → buckets naturally align\n\nTraditional approaches would require rebuilding the entire index when adding data. Krapivin hash tables with fixed seeds enable incremental, compositional updates.\n\n\nHere’s how to query dense buckets directly from the Parquet files:\nimport polars as pl\n\n# Load dense buckets (≥5 items) from Parquet index\ndense = (pl.scan_parquet(\"twitter_lsh_index.parquet\")\n    .group_by('bucket_id')\n    .agg(pl.count('row_id').alias('count'))\n    .filter(pl.col('count') &gt;= 5)\n    .sort('count', descending=True)\n    .collect())\n\nprint(f\"Found {len(dense)} dense buckets\")\n# Found 42 dense buckets\n\n# Get contents of bucket 132 (e.g., \"password reset\" theme)\nbucket_contents = (pl.scan_parquet(\"twitter_lsh_index.parquet\")\n    .filter(pl.col('bucket_id') == 132)\n    .collect())\n\nprint(f\"Bucket 132 contains {len(bucket_contents)} items\")\n# Bucket 132 contains 16 items\nThe key: no scanning required. Parquet’s columnar format + Polars’ lazy evaluation means we only read the columns we need.\n\n\n\n\n\n\n\n\n\nDirect link: semantic-proprioception-demo.streamlit.app\nSource code: github.com/jdonaldson/semantic-proprioception-demo\nSelect a dataset, choose an embedding model, and watch themes emerge automatically. Click into any theme to see the actual text samples that cluster together.\nYou can also: - Compare how different models (MiniLM-L3/L6/L12, MPNet-base) cluster the same data - Adjust the semantic merging threshold to consolidate or separate themes - Search for similar items using both brute-force cosine similarity and LSH-accelerated lookup\n\n\n\nSemantic proprioception isn’t just about visualization—it unlocks new capabilities:\nHallucination detection: If an LLM generates text with high confidence but low embedding density (sparse bucket), it’s likely hallucinating content outside its training distribution.\nActive learning: Sample from sparse regions (novel concepts) or high-entropy buckets (boundary cases) to maximize labeling efficiency.\nContent gap analysis: Compare query density (what users search for) vs. corpus density (what you have) to find opportunities.\nConcept drift detection: Track density distributions over time windows—sudden shifts indicate changing semantics.\n\n\n\nKey papers: - Krapivin et al. (2025): Optimal Hash Tables with Deletion - Indyk & Motwani (1998): Approximate Nearest Neighbors via LSH\n\n\n\nBuilt with: - Streamlit for the interactive UI - Polars for fast DataFrame operations - sentence-transformers (HuggingFace) for embeddings - Krapivin hash tables (Rust + Python bindings) for O(1) density queries - Parquet (zstd compression) for efficient storage\nTotal dataset size: ~24 MB (1,000 tweets + 1,000 papers + 684 HN posts, 4 models each)\n\nThe key insight: data can understand itself. Give it the right structure (LSH + Krapivin), and patterns emerge without manual intervention. Not clustering, not search—semantic self-awareness.\nTry the demo and see what patterns hide in your own data.\n© Copyright 2024 Justin Donaldson. Except where otherwise noted, all rights reserved. The views and opinions on this website are my own and do not represent my current or former employers."
  },
  {
    "objectID": "posts/semantic-proprioception-demo.html#the-core-insight",
    "href": "posts/semantic-proprioception-demo.html#the-core-insight",
    "title": "Semantic Proprioception: Teaching Data to Understand Itself",
    "section": "",
    "text": "Traditional clustering requires you to specify how many clusters you want, tune distance thresholds, or provide seed examples. But what if the data could just tell you what patterns exist?\nThe key is LSH bucket density. When you hash similar embeddings into buckets using Locality-Sensitive Hashing, the density of each bucket reveals something fundamental.\nLSH maps high-dimensional vectors to binary signatures using random hyperplanes:\n\\[h_i(\\mathbf{v}) = \\begin{cases} 1 & \\text{if } \\mathbf{w}_i \\cdot \\mathbf{v} &gt; 0 \\\\ 0 & \\text{otherwise} \\end{cases}\\]\nwhere \\(\\mathbf{w}_i\\) is a random hyperplane. Combining \\(k\\) such hash functions creates a bucket signature. Similar vectors collide in the same bucket with high probability:\n\\[P(h(\\mathbf{u}) = h(\\mathbf{v})) = 1 - \\frac{\\theta(\\mathbf{u}, \\mathbf{v})}{\\pi}\\]\nwhere \\(\\theta(\\mathbf{u}, \\mathbf{v})\\) is the angle between vectors. Closer vectors (smaller angle) → higher collision probability.\nThe density distribution then tells us:\n\nDense buckets (≥5 items): Common themes, frequently occurring concepts\nMedium buckets (2-4 items): Boundary cases, transitional concepts\nSparse buckets (1 item): Novel or rare content\n\nThis isn’t just clustering—it’s the data developing awareness of its own distribution."
  },
  {
    "objectID": "posts/semantic-proprioception-demo.html#why-krapivin-hash-tables-matter",
    "href": "posts/semantic-proprioception-demo.html#why-krapivin-hash-tables-matter",
    "title": "Semantic Proprioception: Teaching Data to Understand Itself",
    "section": "",
    "text": "Traditional LSH implementations have a problem: to find dense buckets, you’d have to scan every bucket and count items. That’s O(n) where n is the number of buckets—expensive and slow.\nEnter Krapivin hash tables (Krapivin et al. 2025), which provide O(1) density queries through their hierarchical structure. You can instantly ask: “Which buckets have ≥5 items?” without scanning anything.\nThis transforms LSH from a search index into a semantic awareness system. The data doesn’t just answer “what’s similar to X?”—it can answer “what patterns exist in me?”"
  },
  {
    "objectID": "posts/semantic-proprioception-demo.html#three-datasets-one-technique",
    "href": "posts/semantic-proprioception-demo.html#three-datasets-one-technique",
    "title": "Semantic Proprioception: Teaching Data to Understand Itself",
    "section": "",
    "text": "The demo shows how the same approach works across wildly different domains:\n\n\nDiscovered themes: Password resets, billing issues, account access, network problems\nThe short, action-oriented nature of support tickets creates tight, well-defined clusters. Users express problems in similar ways, leading to high-density buckets around common pain points.\n\n\n\nDiscovered themes: Deep learning architectures, quantum mechanics, genomics, optimization methods\nAcademic writing has longer, more varied language, but technical concepts still cluster. Papers about “attention mechanisms” use similar terminology even when discussing different applications.\n\n\n\nDiscovered themes: AI/ML developments, startup advice, privacy concerns, programming tools\nHN posts mix news headlines with discussion text. The clusters reflect both trending topics and perennial themes in the tech community."
  },
  {
    "objectID": "posts/semantic-proprioception-demo.html#how-it-works",
    "href": "posts/semantic-proprioception-demo.html#how-it-works",
    "title": "Semantic Proprioception: Teaching Data to Understand Itself",
    "section": "",
    "text": "Embed: Use sentence-transformers to convert text → 384 or 768-dimensional vectors\nHash: Apply LSH with a fixed seed (12345) so embeddings from different files map to the same bucket space\nDiscover: Query Krapivin hash table for buckets with ≥5 items (O(1) operation)\nLabel: Use an LLM or keyword extraction to generate semantic labels for each dense bucket\nMerge: Combine similar themes using Jaccard similarity on tokenized labels:\n\n\\[J(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}\\]\nwhere \\(A\\) and \\(B\\) are sets of tokens from theme labels. Themes with \\(J \\geq 0.5\\) get merged automatically.\nAll embeddings are pre-computed (~24 MB total), so the demo runs with zero API costs or inference overhead."
  },
  {
    "objectID": "posts/semantic-proprioception-demo.html#the-composability-advantage",
    "href": "posts/semantic-proprioception-demo.html#the-composability-advantage",
    "title": "Semantic Proprioception: Teaching Data to Understand Itself",
    "section": "",
    "text": "Because we use a fixed LSH seed across all files, the bucket spaces are compatible. This means:\n\nAdd new data files → just compute their LSH signatures → merge with existing index\nRemove files → delete their entries from affected buckets\nQuery across multiple datasets → buckets naturally align\n\nTraditional approaches would require rebuilding the entire index when adding data. Krapivin hash tables with fixed seeds enable incremental, compositional updates.\n\n\nHere’s how to query dense buckets directly from the Parquet files:\nimport polars as pl\n\n# Load dense buckets (≥5 items) from Parquet index\ndense = (pl.scan_parquet(\"twitter_lsh_index.parquet\")\n    .group_by('bucket_id')\n    .agg(pl.count('row_id').alias('count'))\n    .filter(pl.col('count') &gt;= 5)\n    .sort('count', descending=True)\n    .collect())\n\nprint(f\"Found {len(dense)} dense buckets\")\n# Found 42 dense buckets\n\n# Get contents of bucket 132 (e.g., \"password reset\" theme)\nbucket_contents = (pl.scan_parquet(\"twitter_lsh_index.parquet\")\n    .filter(pl.col('bucket_id') == 132)\n    .collect())\n\nprint(f\"Bucket 132 contains {len(bucket_contents)} items\")\n# Bucket 132 contains 16 items\nThe key: no scanning required. Parquet’s columnar format + Polars’ lazy evaluation means we only read the columns we need."
  },
  {
    "objectID": "posts/semantic-proprioception-demo.html#try-it-yourself",
    "href": "posts/semantic-proprioception-demo.html#try-it-yourself",
    "title": "Semantic Proprioception: Teaching Data to Understand Itself",
    "section": "",
    "text": "Direct link: semantic-proprioception-demo.streamlit.app\nSource code: github.com/jdonaldson/semantic-proprioception-demo\nSelect a dataset, choose an embedding model, and watch themes emerge automatically. Click into any theme to see the actual text samples that cluster together.\nYou can also: - Compare how different models (MiniLM-L3/L6/L12, MPNet-base) cluster the same data - Adjust the semantic merging threshold to consolidate or separate themes - Search for similar items using both brute-force cosine similarity and LSH-accelerated lookup"
  },
  {
    "objectID": "posts/semantic-proprioception-demo.html#what-this-enables",
    "href": "posts/semantic-proprioception-demo.html#what-this-enables",
    "title": "Semantic Proprioception: Teaching Data to Understand Itself",
    "section": "",
    "text": "Semantic proprioception isn’t just about visualization—it unlocks new capabilities:\nHallucination detection: If an LLM generates text with high confidence but low embedding density (sparse bucket), it’s likely hallucinating content outside its training distribution.\nActive learning: Sample from sparse regions (novel concepts) or high-entropy buckets (boundary cases) to maximize labeling efficiency.\nContent gap analysis: Compare query density (what users search for) vs. corpus density (what you have) to find opportunities.\nConcept drift detection: Track density distributions over time windows—sudden shifts indicate changing semantics."
  },
  {
    "objectID": "posts/semantic-proprioception-demo.html#the-research-behind-it",
    "href": "posts/semantic-proprioception-demo.html#the-research-behind-it",
    "title": "Semantic Proprioception: Teaching Data to Understand Itself",
    "section": "",
    "text": "Key papers: - Krapivin et al. (2025): Optimal Hash Tables with Deletion - Indyk & Motwani (1998): Approximate Nearest Neighbors via LSH"
  },
  {
    "objectID": "posts/semantic-proprioception-demo.html#technical-details",
    "href": "posts/semantic-proprioception-demo.html#technical-details",
    "title": "Semantic Proprioception: Teaching Data to Understand Itself",
    "section": "",
    "text": "Built with: - Streamlit for the interactive UI - Polars for fast DataFrame operations - sentence-transformers (HuggingFace) for embeddings - Krapivin hash tables (Rust + Python bindings) for O(1) density queries - Parquet (zstd compression) for efficient storage\nTotal dataset size: ~24 MB (1,000 tweets + 1,000 papers + 684 HN posts, 4 models each)\n\nThe key insight: data can understand itself. Give it the right structure (LSH + Krapivin), and patterns emerge without manual intervention. Not clustering, not search—semantic self-awareness.\nTry the demo and see what patterns hide in your own data."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "Hello! I’m Justin Donaldson, a passionate Human-Computer-Interaction/Data/AI Scientist and Engineer with a diverse skill set spanning large language model research, data science, and machine learning. My journey has led me from academia to industry, and back again. I’m always eager to learn something complex, and turn it into something beautiful.\n© Copyright 2024 Justin Donaldson. Except where otherwise noted, all rights reserved. The views and opinions on this website are my own and do not represent my current or former employers."
  },
  {
    "objectID": "index.html#what-i-know",
    "href": "index.html#what-i-know",
    "title": "About Me",
    "section": "What I know",
    "text": "What I know\n\n\n\n\n\n\nTip\n\n\n\n “All models are wrong, but some are useful”\n- George Box\n\n\nMy expertise encompasses model optimization, time series analysis, anomaly detection, and advanced ML/web visualization. I’m well-versed in programming languages like Python, R, and fluent with Docker and core web technologies. Over the years, I’ve had the opportunity to work on diverse and impactful projects with trusted Salesforce partners, crafting custom solutions ranging from advanced customer behavior modeling for banks to anomaly detection models for global enterprises. Make sure to check out my C.V. for more details.\nMy academic research focused on improving user interaction design and enhancing music recommendation systems through data visualization, social behavior analysis, and acoustic features. Notable studies include creating hybrid recommendation systems combining social and musical data, visualizing social network links for better exploratory search, and exploring aesthetic interaction design. My work also involves emotional experience design and the use of visualizations for music discovery. For further details, visit the publications section\n\n\n\n\n\n\nTip\n\n\n\n “There is no science without fancy and no art without fact.”\n- Vladimir Nabokov\n\n\nMy patents span multiple areas in technology. They include systems for real-time media consumption visualization, decision tree interaction, and machine learning-based software testing. Other patents focus on improving large dataset task performance, secure predictive modeling, personal music recommendation systems, and error assignment in programming. Additionally, my work includes addressing biases in document retrieval and optimizing search rankings through machine learning techniques. These patents collectively aim to improve user interaction, software performance, and data analysis. Visit the patents section for more details.\nI have made notable contributions to education and open-source development. As a co-instructor for UW 410 Advanced Machine Learning, I helped teach advanced machine learning concepts at the University of Washington. I also served on the UW Advisory Board for the School of Professional and Continuing Education. In the open-source space, I developed a Lua target for the Haxe language, which wound up enabling video games such as Pokemon Sword & Shield to run efficiently on the Nintendo Switch. Additionally, I mentored graduate students in data science through UCLA’s program and co-hosted a data science competition with Boston Day Academy. More details in the pro-bono section."
  },
  {
    "objectID": "index.html#dagworks",
    "href": "index.html#dagworks",
    "title": "About Me",
    "section": "Dagworks",
    "text": "Dagworks\n\n\n\nStefan and I\n\n\nI met Stefan and Elijah when they were just getting started building Dagworks.io. They had some of the same notions I had on the importance of Directed Acyclic Graphs as a fundamental programming theme. Their decorator-based approach for static methods creates the necessary coupling between imperative graph steps, and overall graph execution order in an especially elegant way. I believe most people will adopt it at some point if they are using Python to manage data. It’s just that good! You can try Dagworks Hamilton framework online, or check it out on Github. You can also follow their blog.\n\n\n\nDagworks DAG Example\n\n\n\n\n\n\n\n\nNote\n\n\n\n(Update: Dagworks has been acquired by Salesforce, while Hamilton and Burr have been accepted into the Apache Software Foundation as new projects)"
  },
  {
    "objectID": "index.html#pulsespace",
    "href": "index.html#pulsespace",
    "title": "About Me",
    "section": "PulseSpace",
    "text": "PulseSpace\n\n\n\nKarl and I\n\n\nI met Karl Stedman and got a chance to hear about PulseSpace. PulseSpace’s goal is all about providing remote power for satellites. With the growth of the private space industry thanks to SpaceX and Blue Origin, there’s a whole ecosystem of young startups looking to provide service level agreements for orbital infrastructure. PulseSpace is looking to be a sort of “gas station” for satellites that may have malfunctioning or damaged solar panels. Even though I don’t have a background in astrophysics, I think they’re bound to run into all sorts of gnarly software problems that don’t fit easily into conventional industry applications.\n\n\n\nPulse Space"
  },
  {
    "objectID": "index.html#salesforce",
    "href": "index.html#salesforce",
    "title": "About Me",
    "section": "Salesforce",
    "text": "Salesforce\n\n\n\nThe Grenoble Team and I\n\n\nPrior to Hushh.ai, I spent a rewarding decade at Salesforce, where I wore multiple hats as a Principal Data Scientist and Engineer. I had the privilege to lead early search infrastructure modernization projects like migrating the core search infrastructure to model-trained ranking coefficients, and also developed patented deep learning models for detecting error states in Salesforce’s enormous code base. Under Scott Rickard I co-founded the Search/Service Cloud Datascience team and contributed significantly to the creation of an ML education program for engineers. My contributions also extended to investigating and developing GPT-3 technologies."
  },
  {
    "objectID": "index.html#bigml",
    "href": "index.html#bigml",
    "title": "About Me",
    "section": "BigML",
    "text": "BigML\n\n\n\nBigML Visualization\n\n\nEarlier in my career, I co-founded BigML and served as its President from 2011 to 2013. There, I spearheaded the development of interactive model and distribution visualizations and took charge of various administrative tasks, setting the foundation for the company’s growth.\n\n\n\n\n\n\nTip\n\n\n\n “Architects cannot teach nature anything.”\n- Mark Twain"
  },
  {
    "objectID": "posts/book_bot.html",
    "href": "posts/book_bot.html",
    "title": "Creating an LLM Agent for Books",
    "section": "",
    "text": "Book Bot\n© Copyright 2024 Justin Donaldson. Except where otherwise noted, all rights reserved. The views and opinions on this website are my own and do not represent my current or former employers."
  },
  {
    "objectID": "posts/book_bot.html#the-story-behind-the-book",
    "href": "posts/book_bot.html#the-story-behind-the-book",
    "title": "Creating an LLM Agent for Books",
    "section": "The Story Behind the Book",
    "text": "The Story Behind the Book\nThe narrative begins on Black Monday—October 19, 1987. As markets worldwide spiraled downward, my father, Greg Donaldson, watched his clients’ portfolios hemorrhage value. What could have been a career-ending catastrophe instead became a moment of profound insight, triggered by three unexpected phone calls that would reshape his entire approach to investment valuation.\nI realize I’m a biased source of information here, but the resulting book isn’t just another investment manual. It’s a deeply personal journey that weaves together market wisdom, faith, and the raw challenges of building a trustworthy financial practice during times of extreme market volatility. For those interested in the full story, please find the book here."
  },
  {
    "objectID": "posts/book_bot.html#bridging-generations-through-technology",
    "href": "posts/book_bot.html#bridging-generations-through-technology",
    "title": "Creating an LLM Agent for Books",
    "section": "Bridging Generations Through Technology",
    "text": "Bridging Generations Through Technology\nI worked a short while in investment management, but my true calling was in data science (before there was a proper name for it), and large language models (before there was a proper name for it). Using the hal9 framework, I’ve created something unique: an AI agent that is based on my father’s investment philosophy and can engage in detailed discussions about the book’s concepts. This isn’t just another large language model trained on internet data—it’s a specialized interface to a specific body of knowledge of my choosing, maintaining the integrity of the original work while making it interactively accessible."
  },
  {
    "objectID": "posts/book_bot.html#why-this-matters",
    "href": "posts/book_bot.html#why-this-matters",
    "title": "Creating an LLM Agent for Books",
    "section": "Why This Matters",
    "text": "Why This Matters\nIn our fast-paced digital age, we often sacrifice depth for breadth. While general-purpose AI models can speak broadly about many topics, there’s immense value in systems that deeply understand specific, well-vetted sources. This approach creates a more reliable foundation for learning and discussion, especially in fields like investment where accuracy and context are crucial."
  },
  {
    "objectID": "posts/book_bot.html#try-the-rising-income-bot-yourself",
    "href": "posts/book_bot.html#try-the-rising-income-bot-yourself",
    "title": "Creating an LLM Agent for Books",
    "section": "Try the Rising Income Bot Yourself",
    "text": "Try the Rising Income Bot Yourself\nBelow this paragraph, you’ll find an interactive interface where you can ask questions about the book’s concepts and investment philosophies for rising income strategies. While this Rising Income AI agent can provide detailed information about the book’s content and concepts, please note that it cannot and should not provide personalized financial advice. All interactions should be considered educational in nature and not as recommendations for specific investment strategies.\n\n\n\n\n\n\nImportantImportant Legal Information\n\n\n\nThe AI interface provided is for educational and informational purposes only. Neither the AI system nor its responses constitute financial advice, and users should not rely on this system for making investment decisions. All investment strategies involve risk, and past performance does not guarantee future results. Please consult with qualified financial professionals for personalized investment advice tailored to your specific situation.\n\n\n\n\n\n\n\n\nTipPlease be Patient!\n\n\n\nPlease be patient with the bot. It is running free on commodity hardware, and as such, isn’t tuned for performance."
  },
  {
    "objectID": "posts/book_bot.html#looking-forward",
    "href": "posts/book_bot.html#looking-forward",
    "title": "Creating an LLM Agent for Books",
    "section": "Looking Forward",
    "text": "Looking Forward\nThis project represents more than just a technological experiment—it’s a bridge between traditional investment wisdom and modern accessibility. While the underlying technology (running on hardware comparable to a PS5 Pro) is impressive, what truly matters is how it helps preserve and transmit valuable knowledge across generations.\nThe future of financial education might just lie in this blend of time-tested wisdom and cutting-edge technology. By creating focused AI interfaces for specific, high-quality sources, we can maintain the depth and nuance of traditional literature while making it more accessible and interactive for modern learners."
  },
  {
    "objectID": "posts/rumpus.html",
    "href": "posts/rumpus.html",
    "title": "Automated Coding with LLMs: Making a Rumpus",
    "section": "",
    "text": "TODO\n\n\nThis post introduces a simple tool, called rumpus that helps me keep track of TODOs, etc. using the macos menubar. It’s not that interesting on its own. What’s interesting is the fact that it’s written completely using a local LLM in 15 minutes. I wanted to write a quick post on the how and why of it, and how I see programming beginning to change with the increasing power that “off the shelf” LLM models can provide.\nAs a programmer, there are always minor improvements or tweaks I wish I could implement. However, the cost/benefit tradeoff often deters me from spending time on these enhancements. Recently, I’ve been exploring how to integrate large language models (LLMs) into my workflow to streamline this process.\nI prefer keeping reminders in the menubar at the top of my screen for easy access, but I find the flexibility of conventional “Todo” apps lacking. To address this, I started using TODO, FIXME, and other comments throughout my code, often accompanied by emojis. These comments are typically actionable and convey more information than a simple tag or word. My menubar is already pretty crowded enough!\nHere’s a sample piece of code with such comments:\n# TODO: Implement the function to calculate the factorial of a number\ndef factorial(n):\n    # XXX: This is a placeholder implementation\n    if n == 0:\n        return 1\n    else:\n        # FIXME: This recursive call might cause a stack overflow for large n\n        return n * factorial(n - 1)\n\n# TODO: Add proper error handling for invalid input\ndef safe_factorial(n):\n    try:\n        if n &lt; 0:\n            raise ValueError(\"Negative numbers are not allowed\")\n        return factorial(n)\n    except TypeError:\n        print(\"Input must be an integer\")\n    except ValueError as ve:\n        print(ve)\n\n# NOTE: This is a test function to demonstrate the usage of factorial functions\ndef test_factorial():\n    test_cases = [0, 1, 5, -3, 'a']\n    for case in test_cases:\n        print(f\"Factorial of {case}: {safe_factorial(case)}\")\n\n# FIXME: Ensure that the main guard is correctly implemented\nif __name__ == \"__main__\":\n    test_factorial()\nThese comments help track necessary actions across a project. While most IDEs display TODOs in a separate panel, my TODOs are scattered across multiple files, including markdown files that don’t require an editor. For instance, here’s a basic TODO panel from Eclipse. It’s nice, but Eclipse is a memory hog. I don’t want to open it just to see my list.\n\n\n\nExample IDE showing TODOS\n\n\nI wanted a centralized list of these flags visible in the menubar, which is always accessible regardless of the active program.\nThe rump library simplifies menubar configuration, but it requires reading the API documentation and managing basic UI functionality (e.g., showing file matches under the emoji and opening them when clicked). I started with a simple “Hello World” example using rumps, with the help of an LLM:\nimport rumps\n\nclass HelloWorldApp(rumps.App):\n    def __init__(self):\n        super(HelloWorldApp, self).__init__(\"Hello World\")\n\nif __name__ == \"__main__\":\n    HelloWorldApp().run()\nFrom there, it only took a few iterations to develop a script that processes path/extension arguments, searches through files, and tabulates the hits into emoji-based entries in the menubar. The final result looks like this:\n\n\n\nrumpus\n\n\nThis tally of tasks and reminders in my menubar was satisfying to create end-to-end using a library I wanted to work with and an LLM to help compose the functionality. Coding the entire thing took about 15 minutes, far less time than writing this blog post.\nAutomated coding is reaching a point where it can significantly shift the cost/benefit analysis for certain tasks. While there may still be challenges, I believe the resulting script is of higher quality than my usual “15 minute” hacks. I also learned that it’s a good idea to use a combination of libraries and tools as a starting point, rather than just letting the model decide itself what to use.\nThere’s certainly more to be written here, but it’s not bad for 15 minutes of coding!\n\n\n\n© Copyright 2024 Justin Donaldson. Except where otherwise noted, all rights reserved. The views and opinions on this website are my own and do not represent my current or former employers."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "J Justin Donaldson, Ph.D.",
    "section": "",
    "text": "© Copyright 2024 Justin Donaldson. Except where otherwise noted, all rights reserved. The views and opinions on this website are my own and do not represent my current or former employers."
  },
  {
    "objectID": "resume.html#summary",
    "href": "resume.html#summary",
    "title": "J Justin Donaldson, Ph.D.",
    "section": "Summary",
    "text": "Summary\nExperienced Data Scientist and Engineer with a strong background in Large Language Models, Machine Learning, and Advanced Analytics. Proven track record in developing innovative solutions for complex problems in various industries. Ph.D. in Informatics with expertise in visualization, interaction design, and programming language design."
  },
  {
    "objectID": "resume.html#skills",
    "href": "resume.html#skills",
    "title": "J Justin Donaldson, Ph.D.",
    "section": "Skills",
    "text": "Skills\n\nLarge Language Model Research and Applications\nData Science and Advanced Machine Learning\nModeling, Model Optimization, Time Series, Anomaly Detection\nAdvanced ML/Web Visualization\nInteraction Design and Prototyping\nCustomer Service Analysis\nProgramming Language Design\nDocker, Python, R, Web Technologies"
  },
  {
    "objectID": "resume.html#professional-experience",
    "href": "resume.html#professional-experience",
    "title": "J Justin Donaldson, Ph.D.",
    "section": "Professional Experience",
    "text": "Professional Experience\n\nHushh.ai\nCofounder (2024 - Present) - Developed strategies for on-device hybrid LLM search and modeling - Coordinated and managed a large team of R&D interns\n\n\nSalesforce.com\nPrincipal Data Scientist/Engineer, Service Cloud (2014 - 2023) - First Senior Search Data Scientist; migrated core search infrastructure to model-trained ranking coefficients - Developed and patented deep learning models for predicting integration test failures and error assignments - Co-founding member of Service Cloud Data Science team and Foundational AI Components team - Produced initial Docker container configuration for AWS SageMaker deployments - Member of Engineering/Research Leadership Team investigating GPT-3 technologies\nKey Projects with Trusted Partners: - Custom job candidate scoring for CPL (Ireland’s largest recruitment firm) - Advanced candidate profiler for Allegis (4th largest recruitment firm globally) - Anomaly detection model for Hulu’s customer call volume - Predictive customer issue resolution model for Citibank\n\n\nBigML\nCo-founder/President (2011 - 2013) - Developed and patented interactive visualizations for trained models and distributions - Led website development and API integration - Managed administrative tasks as President"
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "J Justin Donaldson, Ph.D.",
    "section": "Education",
    "text": "Education\nPh.D., Informatics - Indiana University, Bloomington (2011) - Dissertation: “Visualization of music relational information sources for analysis, navigation, and discovery” - Best Paper Award\nM.S., Human Computer Interaction Design - Indiana University, Bloomington (2006)\nB.S., Computer Science (Minor in Mathematics) - DePauw University, Greencastle (2001)"
  },
  {
    "objectID": "resume.html#patents",
    "href": "resume.html#patents",
    "title": "J Justin Donaldson, Ph.D.",
    "section": "Patents",
    "text": "Patents\n\nReal-time visualization of user consumption of media items (US8332406B2)\nVisualization and interaction with compact representations of decision trees (US20200379951A1)\nEvolving parallel system to automatically improve the performance of multiple concurrent tasks on large datasets (US9558036B1)\nMachine learning based ranking of test cases for software development (US10474562B2)\nError assignment for computer programs (US10409667B2)\nAccounting for positional bias in a document retrieval system using machine learning (US10565265B2)\n(Additional patents listed in full CV)"
  },
  {
    "objectID": "resume.html#publications",
    "href": "resume.html#publications",
    "title": "J Justin Donaldson, Ph.D.",
    "section": "Publications",
    "text": "Publications\nMultiple peer-reviewed publications in the fields of data science, machine learning, and information visualization (Full list available upon request)"
  },
  {
    "objectID": "resume.html#pro-bono-work",
    "href": "resume.html#pro-bono-work",
    "title": "J Justin Donaldson, Ph.D.",
    "section": "Pro Bono Work",
    "text": "Pro Bono Work\nUniversity of Washington - Co-Instructor for UW 410 Advanced Machine Learning - Member of UW Advisory Board for School of Professional and Continuing Education\nHaxe Foundation - Developed the Lua target for the Haxe language - Sponsored and mentored Summer-of-Code programmers\nUCLA Graduate School - Data Science Advisor/Mentor for graduate student Data Science competition"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Ogni Parte ad Ogni Parte Splende",
    "section": "",
    "text": "“Each part to each part shines”\nThis is the personal blog of Justin Donaldson. You can read the entries below, or learn more about me. You can also check out my CV\n\n\n\nThe Three Sisters Mountains in Sisters, Oregon\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntelligence Chases Chaos\n\n\n\nAI\n\n\n\nThe world we experience is an illusion of order draped over a seething ocean of chaos. Every gust of wind, every market fluctuation, every flicker of thought follows the invisible hand of nonlinear dynamics. The deeper we look, the more we see how Chaos—capital C—dominates reality. But here’s the twist: Chaos isn’t infinite. It’s structured. And that structure is fractal.\n\n\n\n\n\nFeb 3, 2025\n\n\nJustin Donaldson\n\n\n\n\n\n\n\n\n\n\n\n\nCreating an LLM Agent for Books\n\n\n\nAI\n\ninvesting\n\nAgent\n\n\n\nPaper books are often seen as a dinosaur in the era of Generative AI. However, they serve as a vital ‘ground truth’ for human generated content, and AI interfaces into specific books can provide a stable question and answer platform against a static source of ideas.\n\n\n\n\n\nDec 16, 2024\n\n\nJustin Donaldson\n\n\n\n\n\n\n\n\n\n\n\n\nAutomated Coding with LLMs: Making a Rumpus\n\n\n\nLLM\n\nCodeGen\n\n\n\nLLMs have been useful for suggesting small changes to code bases, but can turn frustrating when trying to develop programs holistically.\n\n\n\n\n\nJul 22, 2024\n\n\nJustin Donaldson\n\n\n\n\n\n\n\n\n\n\n\n\nBlogging with Quarto\n\n\n\nblog\n\n\n\nCreating my Webpage with Quarto\n\n\n\n\n\nApr 15, 2024\n\n\nJustin Donaldson\n\n\n\n\n\nNo matching items\n© Copyright 2024 Justin Donaldson. Except where otherwise noted, all rights reserved. The views and opinions on this website are my own and do not represent my current or former employers."
  }
]