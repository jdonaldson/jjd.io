[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "J Justin Donaldson, Ph.D.",
    "section": "",
    "text": "© Copyright 2024 Justin Donaldson. Except where otherwise noted, all rights reserved. The views and opinions on this website are my own and do not represent my current or former employers."
  },
  {
    "objectID": "resume.html#summary",
    "href": "resume.html#summary",
    "title": "J Justin Donaldson, Ph.D.",
    "section": "Summary",
    "text": "Summary\nExperienced Data Scientist and Engineer with a strong background in Large Language Models, Machine Learning, and Advanced Analytics. Proven track record in developing innovative solutions for complex problems in various industries. Ph.D. in Informatics with expertise in visualization, interaction design, and programming language design."
  },
  {
    "objectID": "resume.html#skills",
    "href": "resume.html#skills",
    "title": "J Justin Donaldson, Ph.D.",
    "section": "Skills",
    "text": "Skills\n\nLarge Language Model Research and Applications\nData Science and Advanced Machine Learning\nModeling, Model Optimization, Time Series, Anomaly Detection\nAdvanced ML/Web Visualization\nInteraction Design and Prototyping\nCustomer Service Analysis\nProgramming Language Design\nDocker, Python, R, Web Technologies"
  },
  {
    "objectID": "resume.html#professional-experience",
    "href": "resume.html#professional-experience",
    "title": "J Justin Donaldson, Ph.D.",
    "section": "Professional Experience",
    "text": "Professional Experience\n\nHushh.ai\nCofounder (2024 - Present) - Developed strategies for on-device hybrid LLM search and modeling - Coordinated and managed a large team of R&D interns\n\n\nSalesforce.com\nPrincipal Data Scientist/Engineer, Service Cloud (2014 - 2023) - First Senior Search Data Scientist; migrated core search infrastructure to model-trained ranking coefficients - Developed and patented deep learning models for predicting integration test failures and error assignments - Co-founding member of Service Cloud Data Science team and Foundational AI Components team - Produced initial Docker container configuration for AWS SageMaker deployments - Member of Engineering/Research Leadership Team investigating GPT-3 technologies\nKey Projects with Trusted Partners: - Custom job candidate scoring for CPL (Ireland’s largest recruitment firm) - Advanced candidate profiler for Allegis (4th largest recruitment firm globally) - Anomaly detection model for Hulu’s customer call volume - Predictive customer issue resolution model for Citibank\n\n\nBigML\nCo-founder/President (2011 - 2013) - Developed and patented interactive visualizations for trained models and distributions - Led website development and API integration - Managed administrative tasks as President"
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "J Justin Donaldson, Ph.D.",
    "section": "Education",
    "text": "Education\nPh.D., Informatics - Indiana University, Bloomington (2011) - Dissertation: “Visualization of music relational information sources for analysis, navigation, and discovery” - Best Paper Award\nM.S., Human Computer Interaction Design - Indiana University, Bloomington (2006)\nB.S., Computer Science (Minor in Mathematics) - DePauw University, Greencastle (2001)"
  },
  {
    "objectID": "resume.html#patents",
    "href": "resume.html#patents",
    "title": "J Justin Donaldson, Ph.D.",
    "section": "Patents",
    "text": "Patents\n\nReal-time visualization of user consumption of media items (US8332406B2)\nVisualization and interaction with compact representations of decision trees (US20200379951A1)\nEvolving parallel system to automatically improve the performance of multiple concurrent tasks on large datasets (US9558036B1)\nMachine learning based ranking of test cases for software development (US10474562B2)\nError assignment for computer programs (US10409667B2)\nAccounting for positional bias in a document retrieval system using machine learning (US10565265B2)\n(Additional patents listed in full CV)"
  },
  {
    "objectID": "resume.html#publications",
    "href": "resume.html#publications",
    "title": "J Justin Donaldson, Ph.D.",
    "section": "Publications",
    "text": "Publications\nMultiple peer-reviewed publications in the fields of data science, machine learning, and information visualization (Full list available upon request)"
  },
  {
    "objectID": "resume.html#pro-bono-work",
    "href": "resume.html#pro-bono-work",
    "title": "J Justin Donaldson, Ph.D.",
    "section": "Pro Bono Work",
    "text": "Pro Bono Work\nUniversity of Washington - Co-Instructor for UW 410 Advanced Machine Learning - Member of UW Advisory Board for School of Professional and Continuing Education\nHaxe Foundation - Developed the Lua target for the Haxe language - Sponsored and mentored Summer-of-Code programmers\nUCLA Graduate School - Data Science Advisor/Mentor for graduate student Data Science competition"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "© Copyright 2024 Justin Donaldson. Except where otherwise noted, all rights reserved. The views and opinions on this website are my own and do not represent my current or former employers."
  },
  {
    "objectID": "cv.html#skills",
    "href": "cv.html#skills",
    "title": "Curriculum Vitae",
    "section": "Skills",
    "text": "Skills\n\nLarge Language Model Research and Applications\nData Science and Advanced Machine Learning\nModeling, Model Optimization, Time Series, Anomaly Detection\nAdvanced ML/Web Visualization (Embedding/ML/SVG Related)\nInteraction Design, Prototyping, and Design Theory\nCustomer Service Analysis\nProgramming Language Design\nDocker, Python, R, + most core web technologies and languages."
  },
  {
    "objectID": "cv.html#experience",
    "href": "cv.html#experience",
    "title": "Curriculum Vitae",
    "section": "Experience",
    "text": "Experience\nHushh.ai Cofounder 2024-\nLeadership and Engineering Highlights:\n\nDeveloped strategies for on-device hybrid LLM search and modeling.\nCoordinated and managed a large team of R&D interns.\n\nSalesforce.com Principal Data Scientist/Engineer Service Cloud 2014-2023\nData Science and Engineering Highlights:\n\nFirst Senior Search Datascientist.\nMigrated the core search infrastructure to model-trained ranking coefficients from their previous ad hoc system of boosts and filters.\nDeveloped and patented deep learning models for predicting integration test failures, and error assignments based on code changes.\nCo-founding member of Service Cloud Datascience team.\nPart of a small team of data scientists tasked with creating an ML education program for all Salesforce engineers.\nProduced the initial Docker Container configuration and serving code that is used as the basis for all production AWS Sagemaker deployments.\nCo-founding member of Foundational AI Components team that produced re-usable libraries and components for advanced ML services.\nMember of joint Engineering/Research Leadership Team responsible for investigating and developing related GPT-3 technologies.\n\nC-level projects with Trusted Salesforce Partners:\n\nWrote a custom job candidate scoring function for CPL, Ireland’s largest recruitment firm.\nWrote an advanced candidate profiler for Allegis, The fourth largest recruitment firm in the world.\nWrote an anomaly detection model for Hulu to detect when an isolated event was affecting global customer call volume to an unsustainable degree.\nWorked with Citibank data center team to combine customer financial activity with service interactions in order to drive predictive customer issue resolution.\n\nBigML Co-founder/President 2011-2013\nLeadership and Engineering Highlights\n\nDeveloped and patented interactive visualizations used to represent and explore trained models and distributions.\nManaged various administrative tasks as President (Insurance, Expenses, Office management, etc.)\nLed development of website, and integration with internal/external APIs."
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Curriculum Vitae",
    "section": "Education",
    "text": "Education\nIndiana University - Bloomington, Indiana - 2011\nPh.D, Informatics - January 2011\nMy Erdös Number is 4:\n\nPaul Erdös\nFan Chung\nAlessandro Vespignani\nFilippo Menczer\n(me)\n\nDissertation: “Visualization of music relational information sources for analysis, navigation, and discovery”\nAdvising Committee : Erik A. Stolterman (Chair), Filippo Menczer, Jon Paolillo, Jeff Bardzell, Donald Byrd, Marc Torrens.\nPresident of Graduate Student Body (School of Informatics)\nBest Paper Award (J. Donaldson 2006b)\nIndiana University - Bloomington, Indiana - 2006\nM.S., Human Computer Interaction Design\nDepauw University - Greencastle, Indiana - 2001\nB.S., Computer Science (Minor in Mathematics)"
  },
  {
    "objectID": "cv.html#publications",
    "href": "cv.html#publications",
    "title": "Curriculum Vitae",
    "section": "Publications",
    "text": "Publications\n\n\nBaccigalupo, Claudio, Enric Plaza, and Justin Donaldson. 2008. “Uncovering Affinity of Artists to Multiple Genres from Social Behaviour Data.” In ISMIR, 275–80.\n\n\nDonaldson, Justin. 2006a. “Limestick: Designing for Performer-Audience Connection in Laptop Based Computer Music.” In CHI ’06: CHI ’06 Extended Abstracts on Human Factors in Computing Systems, 712–17. New York, NY, USA: ACM. https://doi.org/http://doi.acm.org/10.1145/1125451.1125595.\n\n\n———. 2006b. “ZMDS: Visualizing Structural Entropy in Queried Network Sub-Graphs.” In. Netsci ’06. Bloomington, IN, USA: Association for Computing Machinery. http://vw.indiana.edu/netsci06/conference/Donaldson_ZMDS_.pdf.\n\n\n———. 2007a. “A Hybrid Social-Acoustic Recommendation System for Popular Music.” In Proceedings of the 2007 ACM Conference on Recommender Systems, 187–90. RecSys ’07. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/1297231.1297271.\n\n\n———. 2007b. “Music Recommendation Mapping and Interface Based on Structural Network Entropy.” In 2007 IEEE 23rd International Conference on Data Engineering Workshop, 811–17. IEEE.\n\n\n———. 2011. “Visualization of Music Relational Information Sources for Analysis, Navigation, and Discovery.” PhD thesis, Indiana University.\n\n\nDonaldson, Justin J, Michael Conover, Benjamin Markines, Heather Roinestad, and Filippo Menczer. 2008. “Visualizing Social Links in Exploratory Search.” In Proceedings of the Nineteenth ACM Conference on Hypertext and Hypermedia, 213–18.\n\n\nDonaldson, Justin, Joshua Evnin, and Sidharth Saxena. 2005. “ECHOES: Encouraging Companionship, Home Organization, and Entertainment in Seniors.” In CHI’05 Extended Abstracts on Human Factors in Computing Systems, 2084–88.\n\n\nDonaldson, Justin, Alla Genkina, Scott MacArthur, Muzaffer Ozakca, and Amanda Stephano. 2004. “DOVE: Digital Olympic Voting Environment.” In CHI’04 Extended Abstracts on Human Factors in Computing Systems, 1631–35.\n\n\nDonaldson, Justin, and William Hazlewood. 2008. “Candidate Mapping: Finding Your Place Amongst the Candidates.” In 2008 12th International Conference Information Visualisation, 63–68. IEEE.\n\n\nDonaldson, Justin, Ian Knopke, and Chris Raphael. 2007. “Chroma Palette: Chromatic Maps of Sound as Granular Synthesis Interface.” In Proceedings of the 7th International Conference on New Interfaces for Musical Expression, 213–18.\n\n\nDonaldson, Justin, and Paul Lamere. 2009. “Using Visualizations for Music Discovery.” Tutorial at ISMIR 9: 99.\n\n\nLim, Youn-kyung, Justin Donaldson, Heekyoung Jung, Breanne Kunz, David Royer, Shruti Ramalingam, Sindhia Thirumaran, and Erik Stolterman. 2008. “Emotional Experience and Interaction Design.” In Affect and Emotion in Human-Computer Interaction: From Theory to Applications, edited by Christian Peter and Russell Beale, 116–29. Berlin, Heidelberg: Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-540-85099-1_10.\n\n\nLim, Youn-kyung, Erik Stolterman, Heekyoung Jung, and Justin Donaldson. 2007a. “Interaction Gestalt and the Design of Aesthetic Interactions.” In Proceedings of the 2007 Conference on Designing Pleasurable Products and Interfaces, 239–54. DPPI ’07. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/1314161.1314183.\n\n\n———. 2007b. “Interaction Gestalt and the Design of Aesthetic Interactions.” In Proceedings of the 2007 Conference on Designing Pleasurable Products and Interfaces, 239–54.\n\n\nMartin, Francisco J., Justin Donaldson, Adam Ashenfelter, Marc Torrens, and Rick Hangartner. 2011. “The Big Promise of Recommender Systems.” AI Magazine 32 (3): 19–27. https://doi.org/10.1609/aimag.v32i3.2360."
  },
  {
    "objectID": "cv.html#patents",
    "href": "cv.html#patents",
    "title": "Curriculum Vitae",
    "section": "Patents",
    "text": "Patents\nJustin Donaldson. Real-time visualization of user consumption of media items. US8332406B2\nJ. Justin Donaldson, Adam Ashenfelter, Francisco Martin, Jos Verwoerd, Jose Antonio Ortega, Charles Parker. Visualization and interaction with compact representations of decision trees.\nUS20200379951A1\nFrancisco J. Martin, Adam Ashenfelter, J. Justin Donaldson, Jos Verwoerd, Jose Antonio Ortega, Charles Parker. Evolving parallel system to automatically improve the performance of multiple concurrent tasks on large datasets. US9558036B1\nFrancisco J. Martin, Oscar Rovira, Jos Verwoerd, Poul Petersen, Charles Parker, Jose Antonio Ortega, Beatriz Garcia, J. Justin Donaldson, Antonio Blasco, Adam Ashenfelter. Predictive modeling and data analysis in a secure shared system. US20170140302A1\nJ. Justin Donaldson, Benjamin Busjaeger, Siddharth Rajaram, Berk Coker, Hormoz Tarevern. Machine learning based ranking of test cases for software development. US10474562B2\nJustin Donaldson. Personal music recommendation mapping WO2008051882A3\nJ. Justin Donaldson, Hormoz Tarevern, Sadiya Hameed, Siddharth Srivastava, Feifei Jiang. Error assignment for computer programs. US10409667B2\nZachary Alexander, Scott Thurston Rickard, Jr., Clifford Z. Huang, J. Justin Donaldson. Accounting for positional bias in a document retrieval system using machine learning. US10565265B2\nScott Thurston Rickard, Jr., Clifford Z. Huang, J. Justin Donaldson. Adjusting feature weights for ranking entity based search results. US20180052853A1"
  },
  {
    "objectID": "cv.html#pro-bono",
    "href": "cv.html#pro-bono",
    "title": "Curriculum Vitae",
    "section": "Pro Bono",
    "text": "Pro Bono\nUniversity of Washington - Instructor/Contributor\n\nCo-Instructor for UW 410 Advanced Machine Learning.\n(Slides Available: https://jdonaldson.github.io/uw-mlearn410/)\nMember of UW Advisory Board for their school of Professional and Continuing Education\n\nHaxe Foundation - Contributor/Member\n\nDeveloped the Lua target for the Haxe language blog announcement | hacker news fp\nSponsored Summer-of-Code for Haxe programmers through mentorship and financial contributions.\n\nUCLA Graduate School - Data Science Advisor/Mentor\n\nCo-hosted a graduate student Data Science competition in collaboration with Boston Day Academy (Slides Available : https://is.gd/qnji8A)"
  },
  {
    "objectID": "posts/book_bot.html",
    "href": "posts/book_bot.html",
    "title": "Creating an LLM Agent for Books",
    "section": "",
    "text": "Book Bot\n© Copyright 2024 Justin Donaldson. Except where otherwise noted, all rights reserved. The views and opinions on this website are my own and do not represent my current or former employers."
  },
  {
    "objectID": "posts/book_bot.html#the-story-behind-the-book",
    "href": "posts/book_bot.html#the-story-behind-the-book",
    "title": "Creating an LLM Agent for Books",
    "section": "The Story Behind the Book",
    "text": "The Story Behind the Book\nThe narrative begins on Black Monday—October 19, 1987. As markets worldwide spiraled downward, my father, Greg Donaldson, watched his clients’ portfolios hemorrhage value. What could have been a career-ending catastrophe instead became a moment of profound insight, triggered by three unexpected phone calls that would reshape his entire approach to investment valuation.\nI realize I’m a biased source of information here, but the resulting book isn’t just another investment manual. It’s a deeply personal journey that weaves together market wisdom, faith, and the raw challenges of building a trustworthy financial practice during times of extreme market volatility. For those interested in the full story, please find the book here."
  },
  {
    "objectID": "posts/book_bot.html#bridging-generations-through-technology",
    "href": "posts/book_bot.html#bridging-generations-through-technology",
    "title": "Creating an LLM Agent for Books",
    "section": "Bridging Generations Through Technology",
    "text": "Bridging Generations Through Technology\nI worked a short while in investment management, but my true calling was in data science (before there was a proper name for it), and large language models (before there was a proper name for it). Using the hal9 framework, I’ve created something unique: an AI agent that is based on my father’s investment philosophy and can engage in detailed discussions about the book’s concepts. This isn’t just another large language model trained on internet data—it’s a specialized interface to a specific body of knowledge of my choosing, maintaining the integrity of the original work while making it interactively accessible."
  },
  {
    "objectID": "posts/book_bot.html#why-this-matters",
    "href": "posts/book_bot.html#why-this-matters",
    "title": "Creating an LLM Agent for Books",
    "section": "Why This Matters",
    "text": "Why This Matters\nIn our fast-paced digital age, we often sacrifice depth for breadth. While general-purpose AI models can speak broadly about many topics, there’s immense value in systems that deeply understand specific, well-vetted sources. This approach creates a more reliable foundation for learning and discussion, especially in fields like investment where accuracy and context are crucial."
  },
  {
    "objectID": "posts/book_bot.html#try-the-rising-income-bot-yourself",
    "href": "posts/book_bot.html#try-the-rising-income-bot-yourself",
    "title": "Creating an LLM Agent for Books",
    "section": "Try the Rising Income Bot Yourself",
    "text": "Try the Rising Income Bot Yourself\nBelow this paragraph, you’ll find an interactive interface where you can ask questions about the book’s concepts and investment philosophies for rising income strategies. While this Rising Income AI agent can provide detailed information about the book’s content and concepts, please note that it cannot and should not provide personalized financial advice. All interactions should be considered educational in nature and not as recommendations for specific investment strategies.\n\n\n\n\n\n\nImportant Legal Information\n\n\n\nThe AI interface provided is for educational and informational purposes only. Neither the AI system nor its responses constitute financial advice, and users should not rely on this system for making investment decisions. All investment strategies involve risk, and past performance does not guarantee future results. Please consult with qualified financial professionals for personalized investment advice tailored to your specific situation.\n\n\n\n\n\n\n\n\nPlease be Patient!\n\n\n\nPlease be patient with the bot. It is running free on commodity hardware, and as such, isn’t tuned for performance."
  },
  {
    "objectID": "posts/book_bot.html#looking-forward",
    "href": "posts/book_bot.html#looking-forward",
    "title": "Creating an LLM Agent for Books",
    "section": "Looking Forward",
    "text": "Looking Forward\nThis project represents more than just a technological experiment—it’s a bridge between traditional investment wisdom and modern accessibility. While the underlying technology (running on hardware comparable to a PS5 Pro) is impressive, what truly matters is how it helps preserve and transmit valuable knowledge across generations.\nThe future of financial education might just lie in this blend of time-tested wisdom and cutting-edge technology. By creating focused AI interfaces for specific, high-quality sources, we can maintain the depth and nuance of traditional literature while making it more accessible and interactive for modern learners."
  },
  {
    "objectID": "posts/intelligence_chases_chaos.html",
    "href": "posts/intelligence_chases_chaos.html",
    "title": "Intelligence Chases Chaos",
    "section": "",
    "text": "Intelligence Chases Chaos\nFractals have an astonishing property: their dimensionality is only a fraction of the space they inhabit. The paths of hurricanes, the spread of wildfires, the beating of the human heart—all of these chaotic systems play out in a space far smaller than their theoretical possibilities.\nOne way of better understanding fractal dimensionality is to understand that it’s not a dimension at all, it’s way of understanding how dimensions change through scaling.\nA good way of understanding this phenomenon is to try and measure a coastline.\nAs one shrinks the resolution of their measurement of a coastline, the length one measures will change, but the amount of change is typically bounded within a given range.\n© Copyright 2024 Justin Donaldson. Except where otherwise noted, all rights reserved. The views and opinions on this website are my own and do not represent my current or former employers."
  },
  {
    "objectID": "posts/intelligence_chases_chaos.html#neural-network-fractals",
    "href": "posts/intelligence_chases_chaos.html#neural-network-fractals",
    "title": "Intelligence Chases Chaos",
    "section": "Neural Network Fractals",
    "text": "Neural Network Fractals\nNeural networks, especially deep learning architectures, actually exhibit fractal-like properties in their information processing and representational spaces. Just as fractals demonstrate non-integer dimensionality by creating complex structures through recursive self-similarity, neural networks generate intricate representations by layering increasingly abstract and self-similar feature transformations.\nConsider convolutional neural networks (CNNs) as an example. As information passes through successive layers, the network progressively extracts features at different scales and levels of abstraction - much like how a fractal reveals different geometric patterns when you zoom in or out. Each layer can be thought of as a kind of “scaling” operation where local patterns are transformed into more global, complex representations. It is a sort of map of the chaos that exists at different levels of abstraction. And, it’s clear that the model is learning some of the same patterns at different levels of abstraction.\n\n\n\nNeural Feature Hierarchy\n\n\nThe fractal-like behavior emerges most clearly in how neural networks handle high-dimensional data. When processing complex inputs like images or language, networks don’t simply map inputs to outputs linearly. Instead, they create multi-dimensional, non-linear transformation spaces where local similarities and global structures coexist - precisely the kind of space where fractal mathematics becomes compelling.\nResearchers like Geoffrey Hinton have even suggested that neural network “feature spaces” might be better understood through fractal geometry than traditional Euclidean approaches. The networks generate representations that are neither purely low-dimensional nor simply high-dimensional, but exist in a kind of fractal in-between state. He gives some of his most recent insight on it here\nMore concretely, techniques like fractal interpolation and fractal compression have direct computational analogues in how neural networks learn and generalize. Both involve recursive strategies for capturing complex patterns at multiple scales of resolution. This holds for images and text in several senses. Transformer models can “hallucinate” parts of an image at any scale, and language models can condense or expand text handily through summary or extrapolation."
  },
  {
    "objectID": "posts/intelligence_chases_chaos.html#the-fractal-shortcut-to-understanding",
    "href": "posts/intelligence_chases_chaos.html#the-fractal-shortcut-to-understanding",
    "title": "Intelligence Chases Chaos",
    "section": "The Fractal Shortcut to Understanding",
    "text": "The Fractal Shortcut to Understanding\nArtificial Intelligence is often accused of being “brute force,” of needing endless data to approximate human intuition. But intelligence—biological or artificial—works because the world isn’t as high-dimensional as it seems. The constraints of physics, economics, and even human behavior force events to unfold along surprisingly low-dimensional manifolds. Chaos, for all its turbulence, is confined to a fractal space. While there may be “No Free Lunch” from a pure statistical point of view, we thankfully live in a statistical world that happens to also be chaotic.\nThis is why deep learning models, trained on quintessentially chaotic market systems, can predict stock trends better than random guessing. It’s why language models, given enough text, can anticipate our next words with uncanny accuracy. They aren’t solving the problem in full dimensionality—they’re following the fractal contours where reality actually unfolds."
  },
  {
    "objectID": "posts/intelligence_chases_chaos.html#ai-as-the-ultimate-chaos-hunter",
    "href": "posts/intelligence_chases_chaos.html#ai-as-the-ultimate-chaos-hunter",
    "title": "Intelligence Chases Chaos",
    "section": "AI as the Ultimate Chaos Hunter",
    "text": "AI as the Ultimate Chaos Hunter\nIf intelligence is about making sense of complexity, then AI is the ultimate tool for chasing Chaos. Traditional physics tries to model the world with explicit equations, but AI sidesteps that limitation. It doesn’t need the equations or full rules of a system—it learns the shape of Chaos directly from observation. Neural networks extract the latent structure from raw events, distilling their fractal dimensionality into something even smaller: a compressed model of reality that understands the large and the small, and how they can relate.\nThis is why AI-driven weather prediction is overtaking classical models. Why AI in finance can detect patterns even seasoned traders miss. And why AI in science is accelerating discoveries faster than human intuition alone ever could.\nIntelligence—ours or artificial—isn’t ultimately about defeating Chaos. It’s about finding its shape in the areas we care about, and learning how to ride its back when necessary. Part of my goal this year is to integrate more fractal consideration of data in my projects this year."
  },
  {
    "objectID": "posts/quarto.html",
    "href": "posts/quarto.html",
    "title": "Blogging with Quarto",
    "section": "",
    "text": "Quarto Example\n\n\nI’m creating a new blog, and was surveying the various options. I’ve been using the default Jekyll “Pages” functionality offered from Github for years, but it’s time to move to something more modern.\nI came across Quarto from a recent post by Jeremy Howard, and was immediately sold. Quarto has its lineage somewhat in the realm of Tex, which is an ancient text setting program written by Donald Knuth, one of the early fathers of computer programming. The Tex computer program eventually evolved into LaTex, which has a scholarly citation management system called BibTeX. Quarto has a mechanism to use these scholarly bibliographic formats, like the ones I maintain for myself when I was writing my dissertation.\nHowever, instead of using the somewhat obtuse LaTex syntax, Quarto uses variations on Markdown, which is a much simpler plain text format for creating basic formatted text. The “qmd” format is easy enough to type out manually with no editor other than a basic terminal (which is also what I use to write these posts). You can see the basic qmd format for this post here.\nI find that I don’t need to write as many mathematical formulas here. Although Quarto enables that easily as well. Here’s one of my favorites, the formula for entropy in information theory.\n\\[H(X) = -\\sum_{i=1}^n p(x_i) \\log p(x_i)\\]\nI can also draw some simple diagrams using a builtin Mermaid syntax:\n\n\n\n\n\nflowchart LR\n  A[Hard edge] --&gt; B(Round edge)\n  B --&gt; C{Decision}\n  C --&gt; D[Result one]\n  C --&gt; E[Result two]\n\n\n\n\n\n\nI can also easily embed youtube clips:\n\nAll in all, most of the basic WYSIWYG formatting work is handled through simple text specification. There’s ways to tweak things even more using css, but for now I’m happy with some basic defaults and theming.\nThere’s two main reasons why I think it’s worth using Quarto:\n\nIt’s Free\nIt doesn’t cost a dime to use Quarto. I simply took the time to set it up with my free Github account, and use it to publish my site free on Github as well.\nNow, there are some that will argue that Quarto is not at all free, in fact it costs time to set this up and write things yourself.\nHowever, I would also argue:\n\n\nIt Has a High “Give a S@&$! Factor”\nWe’re entering an age where most text written online will be generated or influenced by a generative AI. This post has been written with the help of one. However, I believe that human communication requires some actual proof of work… some way of showing that the author cares about the topic. It’s going to get very difficult to do that unless you structure your communication in a way that shows a deep level of interest in a topic, and not just churning out one-off dross for the sake of posting to Linkedin.\nThe main reason isn’t that it’s cheap or shows you care though, it’s because you should own your story as long as you can, and you will probably outlive any company that tries to do too many things for you. This brings up the next advantage of Quarto:\n\n\nIt’s Open Source and Lindy\nTechnology has been around long enough that it should be clear that languages, frameworks, and applications come and go along with the companies that invent them. The concept of the Lindy effect argues that future life expectancy of some non-perishable thing (like an idea or technology) is proportional to its age. Right now, Quarto is built on top of some of the oldest ideas in all of computer theory, and it’s editable and usable inside my Neovim editor, which itself is based off of the VI editor from 1976, using the QWERTY keyboard layout from 1878.\nNone of these technologies are going to go anywhere. They’re good bets for writing things down that you want to control and preserve.\n\n\nThanks for Reading!\nI hope I gave a good overview of what Quarto is about, and why it matters. Thanks for taking the time to read. Hoping to see your Quarto site out there some day!\n\n\n\n\n© Copyright 2024 Justin Donaldson. Except where otherwise noted, all rights reserved. The views and opinions on this website are my own and do not represent my current or former employers."
  },
  {
    "objectID": "notebooks/llama3.1.html",
    "href": "notebooks/llama3.1.html",
    "title": "Justin Donaldson, Ph.D.",
    "section": "",
    "text": "import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\ntorch.backends.quantized.engine = \"qnnpack\"\n# Load the pre-trained model and tokenizer\nmodel_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Apply dynamic quantization to the model\nquantized_model = torch.quantization.quantize_dynamic(\n    model, {torch.nn.Linear}, dtype=torch.qint8\n)\n\n# Create a pipeline with the quantized model\nquantized_pipeline = pipeline(\n    \"text-generation\", \n    model=quantized_model, \n    tokenizer=tokenizer,\n    device = -1\n)\n\nnormal_pipeline = pipeline(\n    \"text-generation\", \n    model=model, \n    tokenizer=tokenizer,\n    device = -1\n)\n\n\n\n\n\ndef get_model_size(model):\n    param_size = 0\n    for param in model.parameters():\n        param_size += param.numel() * param.element_size()\n\n    buffer_size = 0\n    for buffer in model.buffers():\n        buffer_size += buffer.numel() * buffer.element_size()\n\n    size_all_mb = (param_size + buffer_size) / 1024**2\n    return size_all_mb\n\nprint('model', get_model_size(model))\nprint('quantized', get_model_size(quantized_model))\n\nmodel 30633.023681640625\nquantized 2005.023681640625\n\n\n\ndef test_pipeline(pipe, prompt=\"Tell a dad joke that involves socks with sandals\"):\n    # Test the quantized model\n    output = pipe(prompt, max_length=50)\n    print(output)\ntest_pipeline(quantized_pipeline)\ntest_pipeline(normal_pipeline)\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\n[{'generated_text': \"Tell a dad joke that involves socks with sandals\\nHere's one:\\n\\nWhy did the sock go to the party with the sandal?\\n\\nBecause it was a sole-ful occasion! (get it? sole, like the bottom of the foot,\"}]\n[{'generated_text': \"Tell a dad joke that involves socks with sandals\\nHere's one: Why did the sock go with the sandal? Because it was a sole-ful match! (get it? sole-ful, like soulful, but also a reference\"}]\n\n\n\n\n\n© Copyright 2024 Justin Donaldson. Except where otherwise noted, all rights reserved. The views and opinions on this website are my own and do not represent my current or former employers."
  },
  {
    "objectID": "notebooks/Spam Detector.html",
    "href": "notebooks/Spam Detector.html",
    "title": "Download some Libraries",
    "section": "",
    "text": "Warning! AI Generated Content\n\n\n\nDownload some Libraries\n\n!pip install polars floret gdown sentence-transformers tqdm pyarrow altair ipywidgets pandas matplotlib\n\nRequirement already satisfied: polars in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (1.26.0)\nRequirement already satisfied: floret in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (0.10.5)\nRequirement already satisfied: gdown in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (5.2.0)\nRequirement already satisfied: sentence-transformers in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (4.0.1)\nRequirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (4.67.1)\nRequirement already satisfied: pyarrow in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (19.0.1)\nRequirement already satisfied: altair in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (5.5.0)\nRequirement already satisfied: ipywidgets in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (8.1.5)\nRequirement already satisfied: pandas in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (2.2.3)\nCollecting matplotlib\n  Downloading matplotlib-3.10.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (11 kB)\nRequirement already satisfied: numpy in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from floret) (2.2.4)\nRequirement already satisfied: beautifulsoup4 in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from gdown) (4.13.3)\nRequirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from gdown) (3.18.0)\nRequirement already satisfied: requests[socks] in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from gdown) (2.32.3)\nRequirement already satisfied: transformers&lt;5.0.0,&gt;=4.41.0 in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from sentence-transformers) (4.50.2)\nRequirement already satisfied: torch&gt;=1.11.0 in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from sentence-transformers) (2.6.0)\nRequirement already satisfied: scikit-learn in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from sentence-transformers) (1.6.1)\nRequirement already satisfied: scipy in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from sentence-transformers) (1.15.2)\nRequirement already satisfied: huggingface-hub&gt;=0.20.0 in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from sentence-transformers) (0.29.3)\nRequirement already satisfied: Pillow in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from sentence-transformers) (11.1.0)\nRequirement already satisfied: typing_extensions&gt;=4.5.0 in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from sentence-transformers) (4.12.2)\nRequirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from altair) (3.1.5)\nRequirement already satisfied: jsonschema&gt;=3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from altair) (4.23.0)\nRequirement already satisfied: narwhals&gt;=1.14.2 in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from altair) (1.32.0)\nRequirement already satisfied: packaging in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from altair) (24.2)\nRequirement already satisfied: comm&gt;=0.1.3 in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from ipywidgets) (0.2.2)\nRequirement already satisfied: ipython&gt;=6.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from ipywidgets) (8.32.0)\nRequirement already satisfied: traitlets&gt;=4.3.1 in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from ipywidgets) (5.14.3)\nRequirement already satisfied: widgetsnbextension~=4.0.12 in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from ipywidgets) (4.0.13)\nRequirement already satisfied: jupyterlab-widgets~=3.0.12 in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from ipywidgets) (3.0.13)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz&gt;=2020.1 in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata&gt;=2022.7 in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from pandas) (2025.2)\nCollecting contourpy&gt;=1.0.1 (from matplotlib)\n  Using cached contourpy-1.3.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.4 kB)\nCollecting cycler&gt;=0.10 (from matplotlib)\n  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\nCollecting fonttools&gt;=4.22.0 (from matplotlib)\n  Downloading fonttools-4.56.0-cp313-cp313-macosx_10_13_universal2.whl.metadata (101 kB)\nCollecting kiwisolver&gt;=1.3.1 (from matplotlib)\n  Downloading kiwisolver-1.4.8-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.2 kB)\nCollecting pyparsing&gt;=2.3.1 (from matplotlib)\n  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\nRequirement already satisfied: fsspec&gt;=2023.5.0 in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from huggingface-hub&gt;=0.20.0-&gt;sentence-transformers) (2025.3.0)\nRequirement already satisfied: pyyaml&gt;=5.1 in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from huggingface-hub&gt;=0.20.0-&gt;sentence-transformers) (6.0.2)\nRequirement already satisfied: decorator in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets) (5.1.1)\nRequirement already satisfied: jedi&gt;=0.16 in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets) (0.19.2)\nRequirement already satisfied: matplotlib-inline in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets) (0.1.7)\nRequirement already satisfied: pexpect&gt;4.3 in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets) (4.9.0)\nRequirement already satisfied: prompt_toolkit&lt;3.1.0,&gt;=3.0.41 in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets) (3.0.50)\nRequirement already satisfied: pygments&gt;=2.4.0 in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets) (2.19.1)\nRequirement already satisfied: stack_data in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets) (0.6.3)\nRequirement already satisfied: attrs&gt;=22.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from jsonschema&gt;=3.0-&gt;altair) (25.1.0)\nRequirement already satisfied: jsonschema-specifications&gt;=2023.03.6 in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from jsonschema&gt;=3.0-&gt;altair) (2024.10.1)\nRequirement already satisfied: referencing&gt;=0.28.4 in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from jsonschema&gt;=3.0-&gt;altair) (0.36.2)\nRequirement already satisfied: rpds-py&gt;=0.7.1 in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from jsonschema&gt;=3.0-&gt;altair) (0.22.3)\nRequirement already satisfied: six&gt;=1.5 in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas) (1.17.0)\nRequirement already satisfied: networkx in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from torch&gt;=1.11.0-&gt;sentence-transformers) (3.4.2)\nRequirement already satisfied: setuptools in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from torch&gt;=1.11.0-&gt;sentence-transformers) (75.8.0)\nRequirement already satisfied: sympy==1.13.1 in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from torch&gt;=1.11.0-&gt;sentence-transformers) (1.13.1)\nRequirement already satisfied: mpmath&lt;1.4,&gt;=1.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from sympy==1.13.1-&gt;torch&gt;=1.11.0-&gt;sentence-transformers) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from transformers&lt;5.0.0,&gt;=4.41.0-&gt;sentence-transformers) (2024.11.6)\nRequirement already satisfied: tokenizers&lt;0.22,&gt;=0.21 in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from transformers&lt;5.0.0,&gt;=4.41.0-&gt;sentence-transformers) (0.21.1)\nRequirement already satisfied: safetensors&gt;=0.4.3 in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from transformers&lt;5.0.0,&gt;=4.41.0-&gt;sentence-transformers) (0.5.3)\nRequirement already satisfied: soupsieve&gt;1.2 in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from beautifulsoup4-&gt;gdown) (2.6)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from jinja2-&gt;altair) (3.0.2)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from requests[socks]-&gt;gdown) (3.4.1)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from requests[socks]-&gt;gdown) (3.10)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from requests[socks]-&gt;gdown) (2.3.0)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from requests[socks]-&gt;gdown) (2025.1.31)\nRequirement already satisfied: PySocks!=1.5.7,&gt;=1.5.6 in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from requests[socks]-&gt;gdown) (1.7.1)\nRequirement already satisfied: joblib&gt;=1.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from scikit-learn-&gt;sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl&gt;=3.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from scikit-learn-&gt;sentence-transformers) (3.6.0)\nRequirement already satisfied: parso&lt;0.9.0,&gt;=0.8.4 in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from jedi&gt;=0.16-&gt;ipython&gt;=6.1.0-&gt;ipywidgets) (0.8.4)\nRequirement already satisfied: ptyprocess&gt;=0.5 in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from pexpect&gt;4.3-&gt;ipython&gt;=6.1.0-&gt;ipywidgets) (0.7.0)\nRequirement already satisfied: wcwidth in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from prompt_toolkit&lt;3.1.0,&gt;=3.0.41-&gt;ipython&gt;=6.1.0-&gt;ipywidgets) (0.2.13)\nRequirement already satisfied: executing&gt;=1.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from stack_data-&gt;ipython&gt;=6.1.0-&gt;ipywidgets) (2.2.0)\nRequirement already satisfied: asttokens&gt;=2.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from stack_data-&gt;ipython&gt;=6.1.0-&gt;ipywidgets) (3.0.0)\nRequirement already satisfied: pure-eval in /opt/homebrew/Caskroom/miniconda/base/envs/jjd/lib/python3.13/site-packages (from stack_data-&gt;ipython&gt;=6.1.0-&gt;ipywidgets) (0.2.3)\nDownloading matplotlib-3.10.1-cp313-cp313-macosx_11_0_arm64.whl (8.0 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.0/8.0 MB 78.8 MB/s eta 0:00:00\nUsing cached contourpy-1.3.1-cp313-cp313-macosx_11_0_arm64.whl (255 kB)\nUsing cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\nDownloading fonttools-4.56.0-cp313-cp313-macosx_10_13_universal2.whl (2.7 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.7/2.7 MB 87.6 MB/s eta 0:00:00\nDownloading kiwisolver-1.4.8-cp313-cp313-macosx_11_0_arm64.whl (65 kB)\nDownloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\nInstalling collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\nSuccessfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.56.0 kiwisolver-1.4.8 matplotlib-3.10.1 pyparsing-3.2.3\n\n\n\n\nDownload Some Free Spam\n\nimport polars as pl\ndat = pl.read_csv(\"https://raw.githubusercontent.com/bigmlcom/python/refs/heads/master/data/spam.csv\", separator = \"\\t\")\ndat.head()\n\n\nshape: (5, 2)\n\n\n\nType\nMessage\n\n\nstr\nstr\n\n\n\n\n\"ham\"\n\"Go until jurong point, crazy..…\n\n\n\"ham\"\n\"Ok lar... Joking wif u oni...\"\n\n\n\"spam\"\n\"Free entry in 2 a wkly comp to…\n\n\n\"ham\"\n\"U dun say so early hor... U c …\n\n\n\"ham\"\n\"Nah I don't think he goes to u…\n\n\n\n\n\n\n\n\nCreate Message Embeddings\n\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n\ndf = dat.with_columns(   \n    [pl.col(\"Message\").map_elements(model.encode, return_dtype=pl.Object).alias(\"Message_embeddings\")]\n)\ndf.head(1)\n\n\nshape: (1, 3)\n\n\n\nType\nMessage\nMessage_embeddings\n\n\nstr\nstr\nobject\n\n\n\n\n\"ham\"\n\"Go until jurong point, crazy..…\n[-1.69181824e-02 -3.81679200e-02  7.14857653e-02 -5.73437065e-02 -8.66633356e-02  4.65233661e-02  6.01777025e-02 -2.38348581e-02   6.06626458e-02 -7.05441236e-02  5.93964718e-02 -7.71391839e-02 -4.15645130e-02 -3.41324471e-02  8.53624567e-02 -7.76045993e-02   1.13105521e-01 -3.69878896e-02  1.39168883e-02 -4.90012504e-02   1.83193274e-02 -4.10681814e-02 -1.09108746e-01  4.61956151e-02 -1.21960320e-01 -4.33768295e-02 -1.78877972e-02  1.99523382e-03   3.06233601e-03 -7.96412118e-03 -1.33218328e-02  1.67141780e-01 -7.55039454e-02 -1.78407524e-02  1.32588185e-02 -3.15061733e-02 -9.53863785e-02 -8.83596465e-02  7.96378553e-02  6.11798614e-02 -3.47491424e-03  7.61992484e-02 -3.70001234e-02 -2.13697832e-02   1.12011898e-02 -1.18908651e-01  2.00325884e-02  5.57313748e-02   9.57565531e-02 -2.27366723e-02 -5.79422303e-02  4.35438938e-02   2.38019638e-02  5.64076426e-03 -3.35444361e-02 -8.53418722e-04   8.05954728e-03 -3.84804234e-02  3.83039899e-02  4.17757519e-02   4.58615273e-03  3.06468420e-02  5.60260145e-03 -8.75349063e-03 -2.39509307e-02  1.25138625e-03 -4.45523672e-02  6.51425496e-02   1.13622770e-02  1.23233702e-02  4.87583410e-03 -1.38713280e-02 -8.03859252e-03  1.26386555e-02  7.52020627e-02  3.70769165e-02 -5.59643880e-02 -8.63211080e-02 -6.10754788e-02  3.70548405e-02 -4.91377525e-02 -3.20833735e-02  1.90563817e-02 -3.72626409e-02   1.72842620e-03 -1.64863726e-04  1.60731317e-03  1.12929739e-01   6.42531589e-02 -2.71103233e-02  4.35887836e-02  1.85125731e-02 -1.32856295e-01  1.19323749e-02  4.25306447e-02 -3.10521144e-02 -2.45824866e-02  4.72321436e-02 -6.77127242e-02  4.09979597e-02   4.67088893e-02  9.03373137e-02  4.29132208e-02  6.37918562e-02 -1.86576694e-02  1.70605835e-02  7.74438903e-02  5.55131994e-02   5.83152138e-02 -7.39113986e-02 -7.65196234e-02  2.44083293e-02   6.38015196e-02 -1.20272905e-01 -2.81793270e-02 -1.37958815e-02   2.02865135e-02 -1.56714451e-02  5.54748774e-02 -9.40069370e-03   1.18104391e-01  1.41699789e-02 -2.63116732e-02  4.67997789e-02 -1.04309969e-01 -3.17853987e-02 -1.02161113e-02 -1.32458551e-33 -1.99304000e-02 -1.26173627e-03  8.28732848e-02  1.75689452e-03   6.08917512e-02 -4.11049376e-04 -9.47755873e-02 -5.35774678e-02 -4.20552231e-02 -5.85285909e-02 -2.15330888e-02 -1.25193462e-01 -2.87181288e-02 -2.14101449e-02  3.05047128e-02  2.54149064e-02   1.24499738e-01 -6.86813220e-02  1.34712756e-02 -7.46099502e-02   3.00230719e-02 -1.12213850e-01 -6.02861419e-02 -4.42803279e-02 -3.26453298e-02  7.86900148e-02  9.16252658e-03  1.77258067e-02   5.64120896e-02  3.82041037e-02 -4.73747551e-02  1.42837996e-02   3.25910258e-03 -4.89112968e-03 -2.64929198e-02  1.90504678e-02 -7.25003704e-02 -1.86688025e-02  4.56063263e-03 -3.65367979e-02   5.22860326e-02  1.19018525e-01 -4.44218554e-02  5.88492639e-02 -2.44770143e-02  9.23932269e-02  1.11726988e-02  3.72147970e-02   1.18185461e-01 -3.04004606e-02 -6.19890504e-02 -2.62813084e-02 -9.48503315e-02 -2.85046343e-02 -2.49951351e-02  3.44672315e-02 -2.21120268e-02 -6.38141949e-03 -7.25123100e-03 -7.64134061e-03   1.10825926e-01 -2.99052522e-02  3.89806926e-02 -4.21310030e-02   6.89505832e-03  3.36252451e-02 -2.92518288e-02 -3.65735777e-02   5.29934876e-02  3.28635215e-03  3.69590260e-02  2.61338404e-03   8.76879618e-02  4.68911864e-02  6.13586307e-02 -6.01785397e-03   1.88666750e-02 -2.91370470e-02  3.46963890e-02 -2.07888172e-03   6.27531623e-03 -1.33823538e-02 -3.74364331e-02  3.24327461e-02   3.75855044e-02 -2.19553672e-02  5.73135093e-02 -6.04293793e-02   8.59184712e-02 -4.22911206e-03 -6.22577071e-02  2.70021465e-02   4.54709008e-02 -4.48018610e-02 -5.65327071e-02  5.57557312e-34   4.14824523e-02 -3.16414237e-02  8.82135797e-03  2.14761775e-02   3.89992073e-02  2.41183788e-02 -5.67613952e-02  4.12703156e-02   4.11817692e-02  5.81647307e-02 -9.73564982e-02 -3.09904013e-02   9.69219580e-02 -5.61235212e-02  3.41982618e-02  1.07519306e-01   9.06308144e-02 -4.00391631e-02  1.25106135e-02  1.24085620e-01 -6.26365375e-03 -3.65534425e-02  2.52256170e-02  1.07734790e-02 -4.92042229e-02  1.75992511e-02  3.69748212e-02  8.42630640e-02 -6.63305894e-02 -1.66913122e-02 -6.10412173e-02 -3.11728548e-02 -7.21557289e-02 -1.66528746e-02 -1.02617703e-02  1.62422378e-02 -2.06661932e-02  3.92794749e-03 -6.43087970e-03  6.30945563e-02 -5.36041819e-02 -4.91470620e-02  4.55063302e-03  4.88804914e-02   6.50152490e-02  2.36753002e-02  7.23623559e-02  1.58664735e-03 -2.46164910e-02 -7.03328522e-03  6.42405599e-02  3.70860472e-02 -9.38719884e-02  2.34099831e-02  6.17754413e-03  4.73575667e-03 -1.38120297e-02 -4.17801877e-03 -8.14963952e-02 -5.73687106e-02 -3.31521849e-03 -1.86633866e-03 -5.81240878e-02  6.45614713e-02   5.63419983e-02 -1.48974499e-03 -1.90959163e-02  8.73385891e-02 -8.87677968e-02 -6.11231802e-03 -2.63885390e-02  1.62656177e-02 -3.67152281e-02 -3.57657336e-02 -7.20747262e-02  3.37118767e-02 -9.34335310e-03 -1.96274668e-02  2.12872121e-02 -4.48771231e-02   4.40328708e-03  5.67067713e-02 -6.76035210e-02 -2.58699134e-02   6.85539022e-02  6.24181889e-02  1.43683366e-02  9.24333464e-03   5.71860326e-03 -3.62973684e-03 -4.11234498e-02  5.80872372e-02 -3.05861775e-02 -4.40317430e-02 -1.17556658e-04 -2.76080296e-08   3.06018023e-03  1.22044953e-02 -1.19921025e-02  2.68007000e-03   2.42149327e-02 -8.79795104e-02 -4.37452421e-02  1.05309878e-02   6.26235753e-02  1.22771144e-01 -5.61710633e-02 -2.56135575e-02   4.92233858e-02 -4.79623722e-03 -6.59028813e-02  6.13308437e-02   5.94741516e-02 -2.99019776e-02 -1.98101420e-02  1.19421957e-02 -3.48647498e-03  1.16272703e-01  2.60050427e-02 -8.41647312e-02 -3.59536707e-03  1.33070266e-02  3.01273614e-02 -2.32840627e-02   6.92448113e-03 -2.48819869e-02  2.56498531e-03 -2.41190381e-02 -1.07202325e-02 -8.39264765e-02  1.80417206e-02 -3.52585725e-02 -1.23782434e-01 -1.32526588e-02 -3.55782770e-02 -5.06126229e-03   7.70425983e-03 -1.88829061e-02  2.94495281e-02 -3.18319388e-02 -1.80741213e-02  1.31877894e-02  2.83850245e-02 -6.47806143e-03 -1.25165051e-02  2.86001377e-02 -6.50383383e-02 -2.05947123e-02   6.62037684e-03 -3.18170339e-02  6.85986802e-02 -3.24419402e-02 -1.66954510e-02  6.08403757e-02  7.08020255e-02  2.01966185e-02   3.42436433e-02 -6.21646829e-02 -7.35691339e-02 -1.25749502e-03]\n\n\n\n\n\n\n\nimport polars as pl\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n\n# Load your Polars DataFrame (assuming it's already available)\n# Example: df = pl.read_csv(\"your_data.csv\") if reading from a file\n\n# Convert labels to binary (spam = 1, ham = 0) and keep as a Polars expression\ndf = df.with_columns((pl.col(\"Type\") == \"spam\").cast(pl.Int32).alias(\"Label\"))\n\n# Convert Polars columns directly to PyTorch tensors\nX = torch.tensor(df[\"Message_embeddings\"], dtype=torch.float32)  # Embeddings tensor\ny = torch.tensor(df[\"Label\"], dtype=torch.float32).unsqueeze(1)  # Labels tensor\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define logistic regression model\nclass LogisticRegression(nn.Module):\n    def __init__(self, input_dim):\n        super(LogisticRegression, self).__init__()\n        self.linear = nn.Linear(input_dim, 1)  # Single output node\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        return self.sigmoid(self.linear(x))\n\n# Initialize model\ninput_dim = X.shape[1]  # Get embedding size\nmodel = LogisticRegression(input_dim)\n\n# Define loss and optimizer\ncriterion = nn.BCELoss()  # Binary cross-entropy loss\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\n# Training loop\nnum_epochs = 50\nholdout_metrics = []\nfor epoch in range(num_epochs):\n    model.train()\n    optimizer.zero_grad()\n    outputs = model(X_train)\n    loss = criterion(outputs, y_train)\n    loss.backward()\n    optimizer.step()\n\n    if epoch % 10 == 0:\n        # Evaluate the model on the holdout\n        model.eval()\n        with torch.no_grad():\n            y_pred = model(X_test)\n            holdout_loss = criterion(y_pred, y_test)\n            y_pred_labels = (y_pred &gt; 0.5).float()  # Convert probabilities to binary (0 or 1)\n\n        # Convert tensors to NumPy arrays for sklearn metrics\n        y_test_np = y_test.numpy()\n        y_pred_np = y_pred_labels.numpy()\n\n        # Compute evaluation metrics\n        accuracy = accuracy_score(y_test_np, y_pred_np)\n        metrics = {\n            \"epoch\" : epoch,\n            \"loss\" : loss.item(),\n            \"holdout_loss\" : holdout_loss.item(),\n            \"holdout_accuracy\" : accuracy,\n        }\n        holdout_metrics.append(metrics)\n\nmetrics = pd.DataFrame(holdout_metrics).set_index(\"epoch\")\nmetrics\n\n\n\n\n\n\n\n\nloss\nholdout_loss\nholdout_accuracy\n\n\nepoch\n\n\n\n\n\n\n\n0\n0.694177\n0.669596\n0.833333\n\n\n10\n0.489731\n0.485265\n0.878788\n\n\n20\n0.374139\n0.386231\n0.878788\n\n\n30\n0.308182\n0.332601\n0.878788\n\n\n40\n0.264232\n0.297609\n0.886364\n\n\n\n\n\n\n\n\nmetrics.plot(title=\"Holdout Training Metrics by Epoch\");\n\n\n\n\n\n\n\n\n\n\n\n\n© Copyright 2024 Justin Donaldson. Except where otherwise noted, all rights reserved. The views and opinions on this website are my own and do not represent my current or former employers."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "Hello! I’m Justin Donaldson, a passionate Human-Computer-Interaction/Data/AI Scientist and Engineer with a diverse skill set spanning large language model research, data science, and machine learning. My journey has led me from academia to industry, and back again. I’m always eager to learn something complex, and turn it into something beautiful.\n© Copyright 2024 Justin Donaldson. Except where otherwise noted, all rights reserved. The views and opinions on this website are my own and do not represent my current or former employers."
  },
  {
    "objectID": "index.html#what-i-know",
    "href": "index.html#what-i-know",
    "title": "About Me",
    "section": "What I know",
    "text": "What I know\nMy expertise encompasses model optimization, time series analysis, anomaly detection, and advanced ML/web visualization. I’m well-versed in programming languages like Python, R, and fluent with Docker and core web technologies. Over the years, I’ve had the opportunity to work on diverse and impactful projects with trusted Salesforce partners, crafting custom solutions ranging from advanced customer behavior modeling for banks to anomaly detection models for global enterprises. Make sure to check out my C.V. for more details.\nMy academic research focused on improving user interaction design and enhancing music recommendation systems through data visualization, social behavior analysis, and acoustic features. Notable studies include creating hybrid recommendation systems combining social and musical data, visualizing social network links for better exploratory search, and exploring aesthetic interaction design. My work also involves emotional experience design and the use of visualizations for music discovery. For further details, visit the publications section\nMy patents span multiple areas in technology. They include systems for real-time media consumption visualization, decision tree interaction, and machine learning-based software testing. Other patents focus on improving large dataset task performance, secure predictive modeling, personal music recommendation systems, and error assignment in programming. Additionally, my work includes addressing biases in document retrieval and optimizing search rankings through machine learning techniques. These patents collectively aim to improve user interaction, software performance, and data analysis. Visit the patents section for more details.\nI have made notable contributions to education and open-source development. As a co-instructor for UW 410 Advanced Machine Learning, I helped teach advanced machine learning concepts at the University of Washington. I also served on the UW Advisory Board for the School of Professional and Continuing Education. In the open-source space, I developed a Lua target for the Haxe language, which wound up enabling video games such as Pokemon Sword & Shield to run efficiently on the Nintendo Switch. Additionally, I mentored graduate students in data science through UCLA’s program and co-hosted a data science competition with Boston Day Academy. More details in the pro-bono section."
  },
  {
    "objectID": "index.html#dagworks",
    "href": "index.html#dagworks",
    "title": "About Me",
    "section": "Dagworks",
    "text": "Dagworks\n\n\n\nStefan and I\n\n\nI met Stefan and Elijah when they were just getting started building Dagworks.io. They had some of the same notions I had on the importance of Directed Acyclic Graphs as a fundamental programming theme. Their decorator-based approach for static methods creates the necessary coupling between imperative graph steps, and overall graph execution order in an especially elegant way. I believe most people will adopt it at some point if they are using Python to manage data. It’s just that good! You can try Dagworks Hamilton framework online, or check it out on Github. You can also follow their blog.\n\n\n\nDagworks DAG Example"
  },
  {
    "objectID": "index.html#pulsespace",
    "href": "index.html#pulsespace",
    "title": "About Me",
    "section": "PulseSpace",
    "text": "PulseSpace\n\n\n\nKarl and I\n\n\nI met Karl Stedman and got a chance to hear about PulseSpace. PulseSpace’s goal is all about providing remote power for satellites. With the growth of the private space industry thanks to SpaceX and Blue Origin, there’s a whole ecosystem of young startups looking to provide service level agreements for orbital infrastructure. PulseSpace is looking to be a sort of “gas station” for satellites that may have malfunctioning or damaged solar panels. Even though I don’t have a background in astrophysics, I think they’re bound to run into all sorts of gnarly software problems that don’t fit easily into conventional industry applications.\n\n\n\nPulse Space"
  },
  {
    "objectID": "index.html#salesforce",
    "href": "index.html#salesforce",
    "title": "About Me",
    "section": "Salesforce",
    "text": "Salesforce\n\n\n\nThe Grenoble Team and I\n\n\nPrior to Hushh.ai, I spent a rewarding decade at Salesforce, where I wore multiple hats as a Principal Data Scientist and Engineer. I had the privilege to lead early search infrastructure modernization projects like migrating the core search infrastructure to model-trained ranking coefficients, and also developed patented deep learning models for detecting error states in Salesforce’s enormous code base. Under Scott Rickard I co-founded the Search/Service Cloud Datascience team and contributed significantly to the creation of an ML education program for engineers. My contributions also extended to investigating and developing GPT-3 technologies."
  },
  {
    "objectID": "index.html#bigml",
    "href": "index.html#bigml",
    "title": "About Me",
    "section": "BigML",
    "text": "BigML\n\n\n\nBigML Visualization\n\n\nEarlier in my career, I co-founded BigML and served as its President from 2011 to 2013. There, I spearheaded the development of interactive model and distribution visualizations and took charge of various administrative tasks, setting the foundation for the company’s growth."
  },
  {
    "objectID": "posts/From Words To Vectors.html",
    "href": "posts/From Words To Vectors.html",
    "title": "From Words to Vectors: The Journey of Neural Word Embeddings",
    "section": "",
    "text": "What Are Embeddings?\nEmbeddings are numerical representations of objects (such as words, sentences, or images) in a continuous vector space. These representations capture semantic meaning, making it possible for machines to understand and compare them.\n\n\n\n\n\ngraph LR\n    %% Main embedding concepts\n    E[Embeddings] --&gt; SE[Sentence Embeddings]\n    E --&gt; ME[Multi-Modal Embeddings]\n    E --&gt; DE[Document Embeddings]\n    \n    %% Media types\n    MT[Media Types] --&gt; T[Text]\n    MT --&gt; I[Images]\n    MT --&gt; A[Audio]\n    MT --&gt; V[Video]\n    \n    %% Embedding techniques for different media\n    T --&gt; TE[Text Embeddings]\n    T --&gt; SE\n    I --&gt; IE[Image Embeddings]\n    A --&gt; AE[Audio Embeddings]\n    V --&gt; VE[Video Embeddings]\n    \n    %% Relationships between embeddings\n    TE --&gt; ME\n    IE --&gt; ME\n    AE --&gt; ME\n    VE --&gt; ME\n    \n    %% Cross-modal connections\n    ME --&gt; CR[Cross-Modal Retrieval]\n    ME --&gt; MS[Multi-Modal Search]\n    \n    %% Properties\n    style E fill:#f9f,stroke:#333\n    style MT fill:#bbf,stroke:#333\n    style ME fill:#bfb,stroke:#333\n    style CR fill:#fbb,stroke:#333\n    style MS fill:#fbb,stroke:#333\n\n\n\n\n\n\n\nThe core concept of embeddings branches into three main types:\n\nSentence embeddings (specialized for text)\nMulti-modal embeddings (can handle multiple types of media)\nDocument embeddings (for longer text)\n\nDifferent media types (text, images, audio, video) each have their own specialized embedding techniques\nAll these individual embedding types can feed into multi-modal embeddings, which enable:\n\nCross-modal retrieval (finding related content across different media types)\nMulti-modal search (searching across different types of media simultaneously)\n\n\n\n\n\n\n\nflowchart TD\n    subgraph Input[Input Processing]\n        T[Text Input] --&gt; TOK[Tokenization]\n        TOK --&gt; VOC[Vocabulary Creation]\n        VOC --&gt; OHE[One-Hot Encoding]\n    end\n\n    subgraph Training[Training Process]\n        OHE --&gt; NNI[Neural Network Input Layer]\n        NNI --&gt; HID[Hidden Layer]\n        HID --&gt; CTX[Context Learning]\n        \n        subgraph Context[Context Window]\n            CW[Sliding Window]\n            TW[Target Word]\n            SW[Surrounding Words]\n        end\n        \n        CTX --&gt; OPT[Optimization]\n        OPT --&gt; LSF[Loss Function]\n        LSF --&gt; BP[Backpropagation]\n        BP --&gt; UP[Update Weights]\n    end\n\n    subgraph Output[Embedding Result]\n        UP --&gt; WV[Word Vectors]\n        WV --&gt; VS[Vector Space]\n        VS --&gt; SIM[Semantic Similarities]\n        \n        SIM --&gt; |Similar words cluster together| REL[Related Words]\n        SIM --&gt; |Vector arithmetic possible| ANA[Analogies]\n        SIM --&gt; |Distance measures similarity| DIS[Word Relationships]\n    end\n\n    style Input fill:#e1f3ff,stroke:#333\n    style Training fill:#fff3e1,stroke:#333\n    style Output fill:#e1ffe1,stroke:#333\n\n\n\n\n\n\n\n\n\n\n© Copyright 2024 Justin Donaldson. Except where otherwise noted, all rights reserved. The views and opinions on this website are my own and do not represent my current or former employers."
  },
  {
    "objectID": "posts/10_things_mlds_pm.html",
    "href": "posts/10_things_mlds_pm.html",
    "title": "10 Things an AI Project Manager Needs To Know",
    "section": "",
    "text": "Things to Know About AI Management\nA successful AI project manager needs to embrace uncertainties, balancing technical feasibility with business goals. It’s not just about building a model—it’s about understanding when machine learning is the right tool, defining success metrics early, and ensuring models remain useful long after deployment.\nPerhaps most importantly, AI project managers act as the bridge between data scientists, engineers, and business stakeholders. Without strong communication, projects can devolve into a “black box” where decisions are made without clear explanations. To keep everyone aligned, PMs must translate technical complexities into practical insights, ensuring AI delivers real-world impact.\nThis guide covers ten crucial things every AI project manager needs to know, from managing messy data to avoiding costly mistakes.\n© Copyright 2024 Justin Donaldson. Except where otherwise noted, all rights reserved. The views and opinions on this website are my own and do not represent my current or former employers."
  },
  {
    "objectID": "posts/10_things_mlds_pm.html#data-is-everything-and-its-often-messy",
    "href": "posts/10_things_mlds_pm.html#data-is-everything-and-its-often-messy",
    "title": "10 Things an AI Project Manager Needs To Know",
    "section": "1. Data is Everything (And It’s Often Messy)",
    "text": "1. Data is Everything (And It’s Often Messy)\n ML models live and die by the quality of data. Unlike traditional software, where development is more deterministic, ML projects require extensive data collection, cleaning, and preprocessing before even getting to model training. PMs should allocate time and resources for proper data preparation rather than assuming clean datasets exist."
  },
  {
    "objectID": "posts/10_things_mlds_pm.html#models-are-not-magic-they-require-experimentation",
    "href": "posts/10_things_mlds_pm.html#models-are-not-magic-they-require-experimentation",
    "title": "10 Things an AI Project Manager Needs To Know",
    "section": "2. Models Are Not Magic – They Require Experimentation",
    "text": "2. Models Are Not Magic – They Require Experimentation\n ML isn’t just about feeding data into an algorithm and getting perfect results. It involves iterative experimentation, hyperparameter tuning, and evaluation to find the best-performing model. Rushing the process or setting unrealistic deadlines can lead to poor model performance and technical debt."
  },
  {
    "objectID": "posts/10_things_mlds_pm.html#define-success-metrics-early",
    "href": "posts/10_things_mlds_pm.html#define-success-metrics-early",
    "title": "10 Things an AI Project Manager Needs To Know",
    "section": "3. Define Success Metrics Early",
    "text": "3. Define Success Metrics Early\n Project Managers should work with ML teams to define clear, measurable success metrics upfront. Whether it’s accuracy, recall, precision, F1-score, or business KPIs, agreeing on success criteria helps avoid last-minute changes in expectations."
  },
  {
    "objectID": "posts/10_things_mlds_pm.html#ml-models-need-maintenance-and-monitoring",
    "href": "posts/10_things_mlds_pm.html#ml-models-need-maintenance-and-monitoring",
    "title": "10 Things an AI Project Manager Needs To Know",
    "section": "4. ML Models Need Maintenance and Monitoring",
    "text": "4. ML Models Need Maintenance and Monitoring\n Unlike traditional software, ML models degrade over time due to data drift and changing user behavior. Deployment is just the beginning—models require ongoing monitoring, retraining, and evaluation. PMs should plan for post-deployment maintenance and allocate resources accordingly."
  },
  {
    "objectID": "posts/10_things_mlds_pm.html#computational-costs-can-be-high",
    "href": "posts/10_things_mlds_pm.html#computational-costs-can-be-high",
    "title": "10 Things an AI Project Manager Needs To Know",
    "section": "5. Computational Costs Can Be High",
    "text": "5. Computational Costs Can Be High\n Training large ML models, especially deep learning models, requires significant computational resources. GPUs, TPUs, and cloud compute can be expensive. PMs should work with engineers to balance cost, performance, and feasibility when designing ML solutions."
  },
  {
    "objectID": "posts/10_things_mlds_pm.html#explainability-matters-but-its-not-always-easy",
    "href": "posts/10_things_mlds_pm.html#explainability-matters-but-its-not-always-easy",
    "title": "10 Things an AI Project Manager Needs To Know",
    "section": "6. Explainability Matters – But It’s Not Always Easy",
    "text": "6. Explainability Matters – But It’s Not Always Easy\n Stakeholders often ask, “Why did the model make this decision?” While some models (like decision trees) are interpretable, deep learning models can be black boxes. Expecting full explainability from every ML model is unrealistic, but PMs should work with engineers to determine an appropriate level of transparency."
  },
  {
    "objectID": "posts/10_things_mlds_pm.html#not-every-problem-needs-machine-learning",
    "href": "posts/10_things_mlds_pm.html#not-every-problem-needs-machine-learning",
    "title": "10 Things an AI Project Manager Needs To Know",
    "section": "7. Not Every Problem Needs Machine Learning",
    "text": "7. Not Every Problem Needs Machine Learning\n Just because ML is trendy doesn’t mean it’s always the best solution. Many problems can be solved with simpler heuristics, rule-based systems, or traditional analytics. PMs should work with engineers to evaluate whether ML is the right tool for the job rather than assuming it’s always necessary."
  },
  {
    "objectID": "posts/10_things_mlds_pm.html#data-privacy-and-compliance-are-critical",
    "href": "posts/10_things_mlds_pm.html#data-privacy-and-compliance-are-critical",
    "title": "10 Things an AI Project Manager Needs To Know",
    "section": "8. Data Privacy and Compliance Are Critical",
    "text": "8. Data Privacy and Compliance Are Critical\n ML engineers need to follow strict data privacy laws like GDPR and CCPA. PMs should be aware of these regulations and plan projects with compliance in mind, ensuring legal and ethical handling of user data."
  },
  {
    "objectID": "posts/10_things_mlds_pm.html#data-scientists-and-ml-engineers-are-not-the-same",
    "href": "posts/10_things_mlds_pm.html#data-scientists-and-ml-engineers-are-not-the-same",
    "title": "10 Things an AI Project Manager Needs To Know",
    "section": "9. Data Scientists and ML Engineers Are Not the Same",
    "text": "9. Data Scientists and ML Engineers Are Not the Same\n While both roles work with data, data scientists focus on analytics, insights, and modeling, while ML engineers focus on productionizing and scaling models. Understanding this distinction helps PMs allocate tasks more effectively and set realistic expectations."
  },
  {
    "objectID": "posts/10_things_mlds_pm.html#communication-is-key-avoid-the-black-box-syndrome",
    "href": "posts/10_things_mlds_pm.html#communication-is-key-avoid-the-black-box-syndrome",
    "title": "10 Things an AI Project Manager Needs To Know",
    "section": "10. Communication Is Key – Avoid the “Black Box” Syndrome",
    "text": "10. Communication Is Key – Avoid the “Black Box” Syndrome\n ML engineers and data scientists sometimes get deep into technical work, making it hard for PMs and stakeholders to follow. At the same time, vague project requirements can lead to misalignment. Regular check-ins, clear documentation, and collaborative discussions ensure everyone stays on the same page."
  },
  {
    "objectID": "posts/rumpus.html",
    "href": "posts/rumpus.html",
    "title": "Automated Coding with LLMs: Making a Rumpus",
    "section": "",
    "text": "TODO\n\n\nThis post introduces a simple tool, called rumpus that helps me keep track of TODOs, etc. using the macos menubar. It’s not that interesting on its own. What’s interesting is the fact that it’s written completely using a local LLM in 15 minutes. I wanted to write a quick post on the how and why of it, and how I see programming beginning to change with the increasing power that “off the shelf” LLM models can provide.\nAs a programmer, there are always minor improvements or tweaks I wish I could implement. However, the cost/benefit tradeoff often deters me from spending time on these enhancements. Recently, I’ve been exploring how to integrate large language models (LLMs) into my workflow to streamline this process.\nI prefer keeping reminders in the menubar at the top of my screen for easy access, but I find the flexibility of conventional “Todo” apps lacking. To address this, I started using TODO, FIXME, and other comments throughout my code, often accompanied by emojis. These comments are typically actionable and convey more information than a simple tag or word. My menubar is already pretty crowded enough!\nHere’s a sample piece of code with such comments:\n# TODO: Implement the function to calculate the factorial of a number\ndef factorial(n):\n    # XXX: This is a placeholder implementation\n    if n == 0:\n        return 1\n    else:\n        # FIXME: This recursive call might cause a stack overflow for large n\n        return n * factorial(n - 1)\n\n# TODO: Add proper error handling for invalid input\ndef safe_factorial(n):\n    try:\n        if n &lt; 0:\n            raise ValueError(\"Negative numbers are not allowed\")\n        return factorial(n)\n    except TypeError:\n        print(\"Input must be an integer\")\n    except ValueError as ve:\n        print(ve)\n\n# NOTE: This is a test function to demonstrate the usage of factorial functions\ndef test_factorial():\n    test_cases = [0, 1, 5, -3, 'a']\n    for case in test_cases:\n        print(f\"Factorial of {case}: {safe_factorial(case)}\")\n\n# FIXME: Ensure that the main guard is correctly implemented\nif __name__ == \"__main__\":\n    test_factorial()\nThese comments help track necessary actions across a project. While most IDEs display TODOs in a separate panel, my TODOs are scattered across multiple files, including markdown files that don’t require an editor. For instance, here’s a basic TODO panel from Eclipse. It’s nice, but Eclipse is a memory hog. I don’t want to open it just to see my list.\n\n\n\nExample IDE showing TODOS\n\n\nI wanted a centralized list of these flags visible in the menubar, which is always accessible regardless of the active program.\nThe rump library simplifies menubar configuration, but it requires reading the API documentation and managing basic UI functionality (e.g., showing file matches under the emoji and opening them when clicked). I started with a simple “Hello World” example using rumps, with the help of an LLM:\nimport rumps\n\nclass HelloWorldApp(rumps.App):\n    def __init__(self):\n        super(HelloWorldApp, self).__init__(\"Hello World\")\n\nif __name__ == \"__main__\":\n    HelloWorldApp().run()\nFrom there, it only took a few iterations to develop a script that processes path/extension arguments, searches through files, and tabulates the hits into emoji-based entries in the menubar. The final result looks like this:\n\n\n\nrumpus\n\n\nThis tally of tasks and reminders in my menubar was satisfying to create end-to-end using a library I wanted to work with and an LLM to help compose the functionality. Coding the entire thing took about 15 minutes, far less time than writing this blog post.\nAutomated coding is reaching a point where it can significantly shift the cost/benefit analysis for certain tasks. While there may still be challenges, I believe the resulting script is of higher quality than my usual “15 minute” hacks. I also learned that it’s a good idea to use a combination of libraries and tools as a starting point, rather than just letting the model decide itself what to use.\nThere’s certainly more to be written here, but it’s not bad for 15 minutes of coding!\n\n\n\n© Copyright 2024 Justin Donaldson. Except where otherwise noted, all rights reserved. The views and opinions on this website are my own and do not represent my current or former employers."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Ogni Parte ad Ogni Parte Splende",
    "section": "",
    "text": "“Each part to each part shines”\nThis is the personal blog of Justin Donaldson. You can read the entries below, or learn more about me. You can also check out my CV\n\n\n\nThe Three Sisters Mountains in Sisters, Oregon\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom Words to Vectors: The Journey of Neural Word Embeddings\n\n\n\n\n\n\nNeural Networks\n\n\nEmbeddings\n\n\n\nWord embeddings transform individual words into dense numerical vectors in a high-dimensional space, where geometric relationships mirror semantic meaning. Through neural network training that learns from how words appear together in context, the system converts simple tokenized text into rich representations where similar words cluster together and vector arithmetic can reveal complex relationships. This mathematical model of language meaning enables machines to understand the nuanced connections between words that humans grasp intuitively, powering a wide range of natural language processing applications.\n\n\n\n\n\nFeb 14, 2025\n\n\nJustin Donaldson\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things an AI Project Manager Needs To Know\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nBest Practices\n\n\n\nManaging an AI project isn’t like traditional software development—it’s messy, experimental, and requires constant iteration. Data is rarely clean, models need fine-tuning, and deployment is just the beginning. Success comes from setting clear metrics, planning for maintenance, and knowing when ML is the right tool (and when it’s not). Most importantly, strong communication keeps everyone aligned and avoids the dreaded “black box” syndrome.\n\n\n\n\n\nFeb 12, 2025\n\n\nJustin Donaldson\n\n\n\n\n\n\n\n\n\n\n\n\nIntelligence Chases Chaos\n\n\n\n\n\n\nAI\n\n\n\nThe world we experience is an illusion of order draped over a seething ocean of chaos. Every gust of wind, every market fluctuation, every flicker of thought follows the invisible hand of nonlinear dynamics. The deeper we look, the more we see how Chaos—capital C—dominates reality. But here’s the twist: Chaos isn’t infinite. It’s structured. And that structure is fractal.\n\n\n\n\n\nFeb 3, 2025\n\n\nJustin Donaldson\n\n\n\n\n\n\n\n\n\n\n\n\nCreating an LLM Agent for Books\n\n\n\n\n\n\nAI\n\n\ninvesting\n\n\nAgent\n\n\n\nPaper books are often seen as a dinosaur in the era of Generative AI. However, they serve as a vital ‘ground truth’ for human generated content, and AI interfaces into specific books can provide a stable question and answer platform against a static source of ideas.\n\n\n\n\n\nDec 16, 2024\n\n\nJustin Donaldson\n\n\n\n\n\n\n\n\n\n\n\n\nAutomated Coding with LLMs: Making a Rumpus\n\n\n\n\n\n\nLLM\n\n\nCodeGen\n\n\n\nLLMs have been useful for suggesting small changes to code bases, but can turn frustrating when trying to develop programs holistically.\n\n\n\n\n\nJul 22, 2024\n\n\nJustin Donaldson\n\n\n\n\n\n\n\n\n\n\n\n\nBlogging with Quarto\n\n\n\n\n\n\nblog\n\n\n\nCreating my Webpage with Quarto\n\n\n\n\n\nApr 15, 2024\n\n\nJustin Donaldson\n\n\n\n\n\n\nNo matching items\n\n© Copyright 2024 Justin Donaldson. Except where otherwise noted, all rights reserved. The views and opinions on this website are my own and do not represent my current or former employers."
  }
]